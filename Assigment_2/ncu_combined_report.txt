==PROF== Connected to process 2348065 (/home/hice1/mfajeau3/CS8803_GPU/Assigment_2/a.out)
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_global": 0%....50%....100% - 21 passes
[1;32mFUNCTIONAL SUCCESS
[0m[1;34mArray size         :[0m 100000000
[1;34mCPU Sort Time (ms) :[0m 17240.177734
[1;34mGPU Sort Time (ms) :[0m 76923.406250
[1;34mGPU Sort Speed     :[0m 1.299994 million elements per second
[1;31mPERF FAILING
[0m[1;34mGPU Sort is [1;31m4x [1;34mslower than CPU, optimize further!
[1;34mH2D Transfer Time (ms):[0m 0.004064
[1;34mKernel Time (ms)      :[0m 76922.882812
[1;34mD2H Transfer Time (ms):[0m 0.514240
==PROF== Disconnected from process 2348065
[2348065] a.out@127.0.0.1
  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874454
    Memory Throughput                 %        27.52
    DRAM Throughput                   %        27.52
    Duration                         us       583.07
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.33
    SM Active Cycles              cycle    857363.83
    Compute (SM) Throughput           %        58.20
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.52
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8401356
    L2 Hit Rate                            %        33.73
    Mem Pipes Busy                         %        58.20
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.60
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.40
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.21
    Achieved Active Warps Per SM           warp        48.13
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878213
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       585.57
    L1/TEX Cache Throughput           %        11.99
    L2 Cache Throughput               %        34.30
    SM Active Cycles              cycle    863350.77
    Compute (SM) Throughput           %        57.96
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        33.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8397773
    L2 Hit Rate                            %        33.71
    Mem Pipes Busy                         %        57.96
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.09
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.91
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.11
    Achieved Active Warps Per SM           warp        48.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.89%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873086
    Memory Throughput                 %        27.55
    DRAM Throughput                   %        27.55
    Duration                         us       582.08
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.50
    SM Active Cycles              cycle    857226.72
    Compute (SM) Throughput           %        58.25
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.25
    Max Bandwidth                          %        27.55
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8402007
    L2 Hit Rate                            %        33.69
    Mem Pipes Busy                         %        58.25
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.58
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.42
    Active Warps Per Scheduler          warp        12.02
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.02 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.17
    Warp Cycles Per Executed Instruction           cycle        20.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.26
    Achieved Active Warps Per SM           warp        48.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874048
    Memory Throughput                 %        27.52
    DRAM Throughput                   %        27.52
    Duration                         us       582.72
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.24
    SM Active Cycles              cycle    858517.37
    Compute (SM) Throughput           %        58.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.52
    L1/TEX Hit Rate                        %        33.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8393437
    L2 Hit Rate                            %        33.55
    Mem Pipes Busy                         %        58.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.49
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.51
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.14
    Warp Cycles Per Executed Instruction           cycle        20.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.14
    Achieved Active Warps Per SM           warp        48.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878736
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       585.86
    L1/TEX Cache Throughput           %        11.98
    L2 Cache Throughput               %        34.28
    SM Active Cycles              cycle    863554.68
    Compute (SM) Throughput           %        57.87
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        33.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8394196
    L2 Hit Rate                            %        33.66
    Mem Pipes Busy                         %        57.87
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.07
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.93
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.97 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.28
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.11
    Achieved Active Warps Per SM           warp        48.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.89%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874250
    Memory Throughput                 %        27.51
    DRAM Throughput                   %        27.51
    Duration                         us       582.98
    L1/TEX Cache Throughput           %        12.14
    L2 Cache Throughput               %        34.35
    SM Active Cycles              cycle    857019.20
    Compute (SM) Throughput           %        58.22
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.25
    Max Bandwidth                          %        27.51
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8404881
    L2 Hit Rate                            %        33.57
    Mem Pipes Busy                         %        58.22
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.59
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.41
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.27
    Achieved Active Warps Per SM           warp        48.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878316
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       585.60
    L1/TEX Cache Throughput           %        12.01
    L2 Cache Throughput               %        34.30
    SM Active Cycles              cycle    864122.77
    Compute (SM) Throughput           %        57.90
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        32.52
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8391977
    L2 Hit Rate                            %        33.66
    Mem Pipes Busy                         %        57.90
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.99
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.01
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.07
    Achieved Active Warps Per SM           warp        48.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874093
    Memory Throughput                 %        27.52
    DRAM Throughput                   %        27.52
    Duration                         us       582.75
    L1/TEX Cache Throughput           %        12.10
    L2 Cache Throughput               %        34.34
    SM Active Cycles              cycle    859063.60
    Compute (SM) Throughput           %        58.17
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.22
    Max Bandwidth                          %        27.52
    L1/TEX Hit Rate                        %        33.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8392139
    L2 Hit Rate                            %        33.52
    Mem Pipes Busy                         %        58.17
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.55
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.45
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.45%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.14
    Warp Cycles Per Executed Instruction           cycle        20.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.09
    Achieved Active Warps Per SM           warp        48.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.91%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878281
    Memory Throughput                 %        27.39
    DRAM Throughput                   %        27.39
    Duration                         us       585.54
    L1/TEX Cache Throughput           %        11.97
    L2 Cache Throughput               %        34.29
    SM Active Cycles              cycle    864311.56
    Compute (SM) Throughput           %        57.90
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.15
    Max Bandwidth                          %        27.39
    L1/TEX Hit Rate                        %        33.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8396007
    L2 Hit Rate                            %        33.67
    Mem Pipes Busy                         %        57.90
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.07
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.93
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.28
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.02
    Achieved Active Warps Per SM           warp        48.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.98%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873591
    Memory Throughput                 %        27.55
    DRAM Throughput                   %        27.55
    Duration                         us       582.43
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.42
    SM Active Cycles              cycle    857487.09
    Compute (SM) Throughput           %        58.21
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.27
    Max Bandwidth                          %        27.55
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8401402
    L2 Hit Rate                            %        33.62
    Mem Pipes Busy                         %        58.21
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.57
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.43
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.16
    Warp Cycles Per Executed Instruction           cycle        20.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.24
    Achieved Active Warps Per SM           warp        48.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.76%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       872589
    Memory Throughput                 %        27.56
    DRAM Throughput                   %        27.56
    Duration                         us       581.86
    L1/TEX Cache Throughput           %        12.10
    L2 Cache Throughput               %        34.68
    SM Active Cycles              cycle    858534.48
    Compute (SM) Throughput           %        58.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.27
    Max Bandwidth                          %        27.56
    L1/TEX Hit Rate                        %        33.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8398687
    L2 Hit Rate                            %        33.67
    Mem Pipes Busy                         %        58.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.45
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.55
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.89 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.01
    Warp Cycles Per Executed Instruction           cycle        20.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.61
    Achieved Active Warps Per SM           warp        47.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.39%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878397
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       585.63
    L1/TEX Cache Throughput           %        12.01
    L2 Cache Throughput               %        34.30
    SM Active Cycles              cycle    864034.27
    Compute (SM) Throughput           %        57.89
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        32.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8401052
    L2 Hit Rate                            %        33.70
    Mem Pipes Busy                         %        57.89
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.98
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.02
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.02%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.95 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.04
    Achieved Active Warps Per SM           warp        48.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.96%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874059
    Memory Throughput                 %        27.53
    DRAM Throughput                   %        27.53
    Duration                         us       582.75
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.45
    SM Active Cycles              cycle    857707.67
    Compute (SM) Throughput           %        58.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.53
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8401865
    L2 Hit Rate                            %        33.73
    Mem Pipes Busy                         %        58.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.53
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.47
    Active Warps Per Scheduler          warp        12.00
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.00 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.18
    Achieved Active Warps Per SM           warp        48.12
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877476
    Memory Throughput                 %        27.41
    DRAM Throughput                   %        27.41
    Duration                         us       585.02
    L1/TEX Cache Throughput           %        11.99
    L2 Cache Throughput               %        34.34
    SM Active Cycles              cycle    863297.58
    Compute (SM) Throughput           %        57.96
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.41
    L1/TEX Hit Rate                        %        33.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8407507
    L2 Hit Rate                            %        33.69
    Mem Pipes Busy                         %        57.96
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.06
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.94
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.94%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.12
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.88%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874075
    Memory Throughput                 %        27.53
    DRAM Throughput                   %        27.53
    Duration                         us       582.75
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.50
    SM Active Cycles              cycle    857258.06
    Compute (SM) Throughput           %        58.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.22
    Max Bandwidth                          %        27.53
    L1/TEX Hit Rate                        %        33.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8408412
    L2 Hit Rate                            %        33.86
    Mem Pipes Busy                         %        58.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.51
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.49
    Active Warps Per Scheduler          warp        12.00
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.00 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.17
    Warp Cycles Per Executed Instruction           cycle        20.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.20
    Achieved Active Warps Per SM           warp        48.13
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.8%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880961
    Memory Throughput                 %        27.32
    DRAM Throughput                   %        27.32
    Duration                         us       587.36
    L1/TEX Cache Throughput           %        11.83
    L2 Cache Throughput               %        34.38
    SM Active Cycles              cycle    867833.63
    Compute (SM) Throughput           %        57.73
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.32
    L1/TEX Hit Rate                        %        33.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8406171
    L2 Hit Rate                            %        33.68
    Mem Pipes Busy                         %        57.73
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.02
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.98
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.98%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.85 active warps per scheduler, but only an average of 2.67 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.08
    Warp Cycles Per Executed Instruction           cycle        20.09
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.06
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871740
    Memory Throughput                 %        27.60
    DRAM Throughput                   %        27.60
    Duration                         us       581.18
    L1/TEX Cache Throughput           %        12.10
    L2 Cache Throughput               %        34.59
    SM Active Cycles              cycle    858366.57
    Compute (SM) Throughput           %        58.34
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.34
    Max Bandwidth                          %        27.60
    L1/TEX Hit Rate                        %        33.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8408220
    L2 Hit Rate                            %        33.75
    Mem Pipes Busy                         %        58.34
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.48
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.52
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.90 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.01
    Warp Cycles Per Executed Instruction           cycle        20.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.58
    Achieved Active Warps Per SM           warp        47.73
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.42%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878600
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       585.82
    L1/TEX Cache Throughput           %        11.99
    L2 Cache Throughput               %        34.31
    SM Active Cycles              cycle    864795.55
    Compute (SM) Throughput           %        57.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        32.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8408607
    L2 Hit Rate                            %        33.73
    Mem Pipes Busy                         %        57.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.01
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.99
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.99%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.01
    Achieved Active Warps Per SM           warp        48.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.99%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874549
    Memory Throughput                 %        27.53
    DRAM Throughput                   %        27.53
    Duration                         us       583.07
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.23
    SM Active Cycles              cycle    858360.97
    Compute (SM) Throughput           %        58.15
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.53
    L1/TEX Hit Rate                        %        33.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8409814
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        58.15
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.52
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.48
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.12
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.88%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878029
    Memory Throughput                 %        27.41
    DRAM Throughput                   %        27.41
    Duration                         us       585.38
    L1/TEX Cache Throughput           %        11.97
    L2 Cache Throughput               %        34.33
    SM Active Cycles              cycle    864306.72
    Compute (SM) Throughput           %        57.92
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.41
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8412583
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        57.92
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.05
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.95
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.29
    Warp Cycles Per Executed Instruction           cycle        20.29
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.04
    Achieved Active Warps Per SM           warp        48.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.96%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874358
    Memory Throughput                 %        27.54
    DRAM Throughput                   %        27.54
    Duration                         us       582.94
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.23
    SM Active Cycles              cycle    857792.27
    Compute (SM) Throughput           %        58.16
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.54
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8416822
    L2 Hit Rate                            %        33.72
    Mem Pipes Busy                         %        58.16
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.55
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.45
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.45%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.16
    Warp Cycles Per Executed Instruction           cycle        20.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.26
    Achieved Active Warps Per SM           warp        48.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       908725
    Memory Throughput                 %        26.50
    DRAM Throughput                   %        26.50
    Duration                         us       605.86
    L1/TEX Cache Throughput           %        11.55
    L2 Cache Throughput               %        32.76
    SM Active Cycles              cycle    897278.42
    Compute (SM) Throughput           %        55.97
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.61
    Max Bandwidth                          %        26.50
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8410601
    L2 Hit Rate                            %        33.90
    Mem Pipes Busy                         %        55.97
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.57
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.43
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.63 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.57
    Warp Cycles Per Executed Instruction           cycle        20.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.30
    Achieved Active Warps Per SM           warp        46.91
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.7%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880286
    Memory Throughput                 %        27.38
    DRAM Throughput                   %        27.38
    Duration                         us       586.88
    L1/TEX Cache Throughput           %        11.82
    L2 Cache Throughput               %        34.46
    SM Active Cycles              cycle    868459.22
    Compute (SM) Throughput           %        57.77
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.38
    L1/TEX Hit Rate                        %        33.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8431145
    L2 Hit Rate                            %        33.80
    Mem Pipes Busy                         %        57.77
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.01
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.99
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.99%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.85 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.09
    Warp Cycles Per Executed Instruction           cycle        20.09
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.01
    Achieved Active Warps Per SM           warp        47.37
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.99%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871405
    Memory Throughput                 %        27.65
    DRAM Throughput                   %        27.65
    Duration                         us       580.96
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.73
    SM Active Cycles              cycle    858056.94
    Compute (SM) Throughput           %        58.35
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.32
    Max Bandwidth                          %        27.65
    L1/TEX Hit Rate                        %        33.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8418053
    L2 Hit Rate                            %        33.75
    Mem Pipes Busy                         %        58.35
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.43
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.57
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.89 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.01
    Warp Cycles Per Executed Instruction           cycle        20.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.64
    Achieved Active Warps Per SM           warp        47.77
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877724
    Memory Throughput                 %        27.45
    DRAM Throughput                   %        27.45
    Duration                         us       585.15
    L1/TEX Cache Throughput           %        12.01
    L2 Cache Throughput               %        34.36
    SM Active Cycles              cycle    863805.27
    Compute (SM) Throughput           %        57.94
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.17
    Max Bandwidth                          %        27.45
    L1/TEX Hit Rate                        %        32.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8419779
    L2 Hit Rate                            %        33.78
    Mem Pipes Busy                         %        57.94
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.98
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.02
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         2.73
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.02%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.95 active warps per scheduler, but only an average of 2.73 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.08
    Achieved Active Warps Per SM           warp        48.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.92%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873769
    Memory Throughput                 %        27.59
    DRAM Throughput                   %        27.59
    Duration                         us       582.56
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.56
    SM Active Cycles              cycle    857724.81
    Compute (SM) Throughput           %        58.20
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.26
    Max Bandwidth                          %        27.59
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8417309
    L2 Hit Rate                            %        33.81
    Mem Pipes Busy                         %        58.20
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.53
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.47
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.13
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.87%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877576
    Memory Throughput                 %        27.46
    DRAM Throughput                   %        27.46
    Duration                         us       585.06
    L1/TEX Cache Throughput           %        11.97
    L2 Cache Throughput               %        34.37
    SM Active Cycles              cycle    864113.32
    Compute (SM) Throughput           %        57.95
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.18
    Max Bandwidth                          %        27.46
    L1/TEX Hit Rate                        %        33.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8454056
    L2 Hit Rate                            %        33.81
    Mem Pipes Busy                         %        57.95
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.08
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.92
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.97 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.07
    Achieved Active Warps Per SM           warp        48.04
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873515
    Memory Throughput                 %        27.60
    DRAM Throughput                   %        27.60
    Duration                         us       582.40
    L1/TEX Cache Throughput           %        12.14
    L2 Cache Throughput               %        34.55
    SM Active Cycles              cycle    856475.10
    Compute (SM) Throughput           %        58.22
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.28
    Max Bandwidth                          %        27.60
    L1/TEX Hit Rate                        %        33.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8425035
    L2 Hit Rate                            %        33.79
    Mem Pipes Busy                         %        58.22
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.57
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.43
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.17
    Warp Cycles Per Executed Instruction           cycle        20.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.27
    Achieved Active Warps Per SM           warp        48.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       911431
    Memory Throughput                 %        26.44
    DRAM Throughput                   %        26.44
    Duration                         us       607.65
    L1/TEX Cache Throughput           %        11.40
    L2 Cache Throughput               %        32.64
    SM Active Cycles              cycle    902830.16
    Compute (SM) Throughput           %        55.80
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.56
    Max Bandwidth                          %        26.44
    L1/TEX Hit Rate                        %        33.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8455310
    L2 Hit Rate                            %        33.80
    Mem Pipes Busy                         %        55.80
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.09
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.91
    Active Warps Per Scheduler          warp        11.70
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.70 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.87
    Warp Cycles Per Executed Instruction           cycle        20.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.69
    Achieved Active Warps Per SM           warp        47.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.31%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       907098
    Memory Throughput                 %        26.54
    DRAM Throughput                   %        26.54
    Duration                         us       604.80
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %        32.72
    SM Active Cycles              cycle    897331.07
    Compute (SM) Throughput           %        56.07
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.63
    Max Bandwidth                          %        26.54
    L1/TEX Hit Rate                        %        33.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8456170
    L2 Hit Rate                            %        33.92
    Mem Pipes Busy                         %        56.07
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.50
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.50
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.60
    Warp Cycles Per Executed Instruction           cycle        20.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.26
    Achieved Active Warps Per SM           warp        46.89
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880641
    Memory Throughput                 %        27.35
    DRAM Throughput                   %        27.35
    Duration                         us       587.14
    L1/TEX Cache Throughput           %        11.83
    L2 Cache Throughput               %        34.47
    SM Active Cycles              cycle    867581.14
    Compute (SM) Throughput           %        57.75
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.35
    L1/TEX Hit Rate                        %        33.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8455795
    L2 Hit Rate                            %        33.87
    Mem Pipes Busy                         %        57.75
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.08
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.92
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.86 active warps per scheduler, but only an average of 2.67 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.08
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.07
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871623
    Memory Throughput                 %        27.63
    DRAM Throughput                   %        27.63
    Duration                         us       581.12
    L1/TEX Cache Throughput           %        12.10
    L2 Cache Throughput               %        34.64
    SM Active Cycles              cycle    858640.48
    Compute (SM) Throughput           %        58.34
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.30
    Max Bandwidth                          %        27.63
    L1/TEX Hit Rate                        %        33.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8455861
    L2 Hit Rate                            %        33.75
    Mem Pipes Busy                         %        58.34
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.39
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.61
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.61%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.89 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.02
    Warp Cycles Per Executed Instruction           cycle        20.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.61
    Achieved Active Warps Per SM           warp        47.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.39%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878989
    Memory Throughput                 %        27.40
    DRAM Throughput                   %        27.40
    Duration                         us       586.05
    L1/TEX Cache Throughput           %        12.00
    L2 Cache Throughput               %        34.36
    SM Active Cycles              cycle    864354.23
    Compute (SM) Throughput           %        57.86
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.40
    L1/TEX Hit Rate                        %        32.62
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8457300
    L2 Hit Rate                            %        33.87
    Mem Pipes Busy                         %        57.86
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.99
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.01
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.28
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.02
    Achieved Active Warps Per SM           warp        48.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.98%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874627
    Memory Throughput                 %        27.54
    DRAM Throughput                   %        27.54
    Duration                         us       583.10
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.68
    SM Active Cycles              cycle    858191.13
    Compute (SM) Throughput           %        58.14
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.28
    Max Bandwidth                          %        27.54
    L1/TEX Hit Rate                        %        33.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8457117
    L2 Hit Rate                            %        33.86
    Mem Pipes Busy                         %        58.14
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.53
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.47
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.14
    Warp Cycles Per Executed Instruction           cycle        20.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.09
    Achieved Active Warps Per SM           warp        48.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.91%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877642
    Memory Throughput                 %        27.44
    DRAM Throughput                   %        27.44
    Duration                         us       585.09
    L1/TEX Cache Throughput           %        11.98
    L2 Cache Throughput               %        34.40
    SM Active Cycles              cycle    863584.23
    Compute (SM) Throughput           %        57.94
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.19
    Max Bandwidth                          %        27.44
    L1/TEX Hit Rate                        %        33.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8457935
    L2 Hit Rate                            %        33.86
    Mem Pipes Busy                         %        57.94
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.08
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.92
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.97 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.09
    Achieved Active Warps Per SM           warp        48.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.91%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873714
    Memory Throughput                 %        27.57
    DRAM Throughput                   %        27.57
    Duration                         us       582.56
    L1/TEX Cache Throughput           %        12.14
    L2 Cache Throughput               %        34.74
    SM Active Cycles              cycle    856849.80
    Compute (SM) Throughput           %        58.25
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.27
    Max Bandwidth                          %        27.57
    L1/TEX Hit Rate                        %        33.52
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8463567
    L2 Hit Rate                            %        33.95
    Mem Pipes Busy                         %        58.25
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.59
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.41
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.16
    Warp Cycles Per Executed Instruction           cycle        20.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.24
    Achieved Active Warps Per SM           warp        48.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.76%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       907731
    Memory Throughput                 %        26.49
    DRAM Throughput                   %        26.49
    Duration                         us       605.18
    L1/TEX Cache Throughput           %        11.51
    L2 Cache Throughput               %        32.84
    SM Active Cycles              cycle    898371.12
    Compute (SM) Throughput           %        56.03
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.59
    Max Bandwidth                          %        26.49
    L1/TEX Hit Rate                        %        33.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8455831
    L2 Hit Rate                            %        33.87
    Mem Pipes Busy                         %        56.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.35
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.65
    Active Warps Per Scheduler          warp        11.67
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.67 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.72
    Warp Cycles Per Executed Instruction           cycle        20.72
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        47.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.33%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       912294
    Memory Throughput                 %        26.31
    DRAM Throughput                   %        26.31
    Duration                         us       608.22
    L1/TEX Cache Throughput           %        11.35
    L2 Cache Throughput               %        32.47
    SM Active Cycles              cycle    906573.48
    Compute (SM) Throughput           %        55.75
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.49
    Max Bandwidth                          %        26.31
    L1/TEX Hit Rate                        %        33.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8390450
    L2 Hit Rate                            %        33.64
    Mem Pipes Busy                         %        55.75
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.21
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.79
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.79%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.79
    Warp Cycles Per Executed Instruction           cycle        20.79
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.55%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.6% of the total average of 20.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        47.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       907743
    Memory Throughput                 %        26.45
    DRAM Throughput                   %        26.45
    Duration                         us       605.18
    L1/TEX Cache Throughput           %        11.55
    L2 Cache Throughput               %        32.55
    SM Active Cycles              cycle    898445.83
    Compute (SM) Throughput           %        56.03
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.61
    Max Bandwidth                          %        26.45
    L1/TEX Hit Rate                        %        33.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8411730
    L2 Hit Rate                            %        33.71
    Mem Pipes Busy                         %        56.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.54
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.46
    Active Warps Per Scheduler          warp        11.62
    Eligible Warps Per Scheduler        warp         2.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.46%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.62 active warps per scheduler, but only an average of 2.63 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.56
    Warp Cycles Per Executed Instruction           cycle        20.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.22
    Achieved Active Warps Per SM           warp        46.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880666
    Memory Throughput                 %        27.28
    DRAM Throughput                   %        27.28
    Duration                         us       587.14
    L1/TEX Cache Throughput           %        11.83
    L2 Cache Throughput               %        34.42
    SM Active Cycles              cycle    867425.60
    Compute (SM) Throughput           %        57.75
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.13
    Max Bandwidth                          %        27.28
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8415040
    L2 Hit Rate                            %        33.70
    Mem Pipes Busy                         %        57.75
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.10
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.90
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.9%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.86 active warps per scheduler, but only an average of 2.67 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.06
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.09
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.91%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871824
    Memory Throughput                 %        27.57
    DRAM Throughput                   %        27.57
    Duration                         us       581.25
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.52
    SM Active Cycles              cycle    858221.23
    Compute (SM) Throughput           %        58.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.30
    Max Bandwidth                          %        27.57
    L1/TEX Hit Rate                        %        33.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8430418
    L2 Hit Rate                            %        33.68
    Mem Pipes Busy                         %        58.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.45
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.55
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.90 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.01
    Warp Cycles Per Executed Instruction           cycle        20.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.59
    Achieved Active Warps Per SM           warp        47.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878955
    Memory Throughput                 %        27.35
    DRAM Throughput                   %        27.35
    Duration                         us       586.08
    L1/TEX Cache Throughput           %        12.00
    L2 Cache Throughput               %        34.32
    SM Active Cycles              cycle    864158.30
    Compute (SM) Throughput           %        57.90
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.35
    L1/TEX Hit Rate                        %        32.52
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8430091
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        57.90
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.00
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.00
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41%                                                                                       
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.03
    Achieved Active Warps Per SM           warp        48.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.97%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       875427
    Memory Throughput                 %        27.45
    DRAM Throughput                   %        27.45
    Duration                         us       583.74
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.52
    SM Active Cycles              cycle    857680.92
    Compute (SM) Throughput           %        58.14
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.21
    Max Bandwidth                          %        27.45
    L1/TEX Hit Rate                        %        33.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8428492
    L2 Hit Rate                            %        33.79
    Mem Pipes Busy                         %        58.14
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.54
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.46
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.46%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.14
    Warp Cycles Per Executed Instruction           cycle        20.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.14
    Achieved Active Warps Per SM           warp        48.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878505
    Memory Throughput                 %        27.37
    DRAM Throughput                   %        27.37
    Duration                         us       585.73
    L1/TEX Cache Throughput           %        11.97
    L2 Cache Throughput               %        34.34
    SM Active Cycles              cycle    863962.36
    Compute (SM) Throughput           %        57.89
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.15
    Max Bandwidth                          %        27.37
    L1/TEX Hit Rate                        %        33.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8430241
    L2 Hit Rate                            %        33.77
    Mem Pipes Busy                         %        57.89
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.11
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.89
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.04
    Achieved Active Warps Per SM           warp        48.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.96%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873254
    Memory Throughput                 %        27.54
    DRAM Throughput                   %        27.54
    Duration                         us       582.18
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.47
    SM Active Cycles              cycle    856827.74
    Compute (SM) Throughput           %        58.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.27
    Max Bandwidth                          %        27.54
    L1/TEX Hit Rate                        %        33.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8430225
    L2 Hit Rate                            %        33.78
    Mem Pipes Busy                         %        58.23
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.60
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.40
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.16
    Warp Cycles Per Executed Instruction           cycle        20.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.27
    Achieved Active Warps Per SM           warp        48.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       902127
    Memory Throughput                 %        26.60
    DRAM Throughput                   %        26.60
    Duration                         us       601.47
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %        32.87
    SM Active Cycles              cycle    896829.57
    Compute (SM) Throughput           %        56.37
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.31
    Mem Busy                               %        18.71
    Max Bandwidth                          %        26.60
    L1/TEX Hit Rate                        %        33.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8440067
    L2 Hit Rate                            %        33.84
    Mem Pipes Busy                         %        56.37
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.44
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.56
    Active Warps Per Scheduler          warp        11.67
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.67 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.67
    Warp Cycles Per Executed Instruction           cycle        20.67
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.76
    Achieved Active Warps Per SM           warp        47.21
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       905822
    Memory Throughput                 %        26.40
    DRAM Throughput                   %        26.40
    Duration                         us       603.90
    L1/TEX Cache Throughput           %        11.51
    L2 Cache Throughput               %        32.64
    SM Active Cycles              cycle    897400.06
    Compute (SM) Throughput           %        56.14
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.55
    Max Bandwidth                          %        26.40
    L1/TEX Hit Rate                        %        33.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8340349
    L2 Hit Rate                            %        33.58
    Mem Pipes Busy                         %        56.14
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.47
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.53
    Active Warps Per Scheduler          warp        11.68
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.68 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.69
    Warp Cycles Per Executed Instruction           cycle        20.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        47.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.3%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       913200
    Memory Throughput                 %        26.24
    DRAM Throughput                   %        26.24
    Duration                         us       608.83
    L1/TEX Cache Throughput           %        11.41
    L2 Cache Throughput               %        32.48
    SM Active Cycles              cycle    901777.04
    Compute (SM) Throughput           %        55.69
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.46
    Max Bandwidth                          %        26.24
    L1/TEX Hit Rate                        %        33.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8397449
    L2 Hit Rate                            %        33.69
    Mem Pipes Busy                         %        55.69
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.11
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.89
    Active Warps Per Scheduler          warp        11.70
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.70 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.85
    Warp Cycles Per Executed Instruction           cycle        20.85
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.76
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       909250
    Memory Throughput                 %        26.39
    DRAM Throughput                   %        26.39
    Duration                         us       606.30
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %        32.58
    SM Active Cycles              cycle    897331.82
    Compute (SM) Throughput           %        55.99
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.30
    Mem Busy                               %        18.58
    Max Bandwidth                          %        26.39
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8415748
    L2 Hit Rate                            %        33.74
    Mem Pipes Busy                         %        55.99
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.68
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.32
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.63 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.53
    Warp Cycles Per Executed Instruction           cycle        20.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        46.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.68%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880131
    Memory Throughput                 %        27.25
    DRAM Throughput                   %        27.25
    Duration                         us       586.78
    L1/TEX Cache Throughput           %        11.85
    L2 Cache Throughput               %        34.44
    SM Active Cycles              cycle    866466.88
    Compute (SM) Throughput           %        57.78
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.12
    Max Bandwidth                          %        27.25
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8415497
    L2 Hit Rate                            %        33.77
    Mem Pipes Busy                         %        57.78
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.09
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.91
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.68
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.86 active warps per scheduler, but only an average of 2.68 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.14
    Achieved Active Warps Per SM           warp        47.45
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871831
    Memory Throughput                 %        27.52
    DRAM Throughput                   %        27.52
    Duration                         us       581.25
    L1/TEX Cache Throughput           %        12.09
    L2 Cache Throughput               %        34.54
    SM Active Cycles              cycle    858849.33
    Compute (SM) Throughput           %        58.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.26
    Max Bandwidth                          %        27.52
    L1/TEX Hit Rate                        %        33.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8428204
    L2 Hit Rate                            %        33.91
    Mem Pipes Busy                         %        58.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.43
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.57
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.90 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.02
    Warp Cycles Per Executed Instruction           cycle        20.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.59
    Achieved Active Warps Per SM           warp        47.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878116
    Memory Throughput                 %        27.32
    DRAM Throughput                   %        27.32
    Duration                         us       585.44
    L1/TEX Cache Throughput           %        12.01
    L2 Cache Throughput               %        34.37
    SM Active Cycles              cycle    863692.22
    Compute (SM) Throughput           %        57.91
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.32
    L1/TEX Hit Rate                        %        32.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8429491
    L2 Hit Rate                            %        33.80
    Mem Pipes Busy                         %        57.91
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.02
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.98
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.98%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.06
    Achieved Active Warps Per SM           warp        48.04
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874535
    Memory Throughput                 %        27.45
    DRAM Throughput                   %        27.45
    Duration                         us       583.04
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.36
    SM Active Cycles              cycle    857543.54
    Compute (SM) Throughput           %        58.15
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.22
    Max Bandwidth                          %        27.45
    L1/TEX Hit Rate                        %        33.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8427930
    L2 Hit Rate                            %        33.79
    Mem Pipes Busy                         %        58.15
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.52
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.48
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.13
    Warp Cycles Per Executed Instruction           cycle        20.13
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.12
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.88%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877804
    Memory Throughput                 %        27.33
    DRAM Throughput                   %        27.33
    Duration                         us       585.22
    L1/TEX Cache Throughput           %        11.97
    L2 Cache Throughput               %        34.37
    SM Active Cycles              cycle    863850.10
    Compute (SM) Throughput           %        57.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.14
    Max Bandwidth                          %        27.33
    L1/TEX Hit Rate                        %        33.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8421351
    L2 Hit Rate                            %        33.82
    Mem Pipes Busy                         %        57.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.05
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.95
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.97 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.27
    Warp Cycles Per Executed Instruction           cycle        20.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.07
    Achieved Active Warps Per SM           warp        48.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       873865
    Memory Throughput                 %        27.44
    DRAM Throughput                   %        27.44
    Duration                         us       582.62
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.57
    SM Active Cycles              cycle    856782.92
    Compute (SM) Throughput           %        58.19
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.24
    Max Bandwidth                          %        27.44
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8413522
    L2 Hit Rate                            %        33.78
    Mem Pipes Busy                         %        58.19
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.56
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.44
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.17
    Warp Cycles Per Executed Instruction           cycle        20.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.25
    Achieved Active Warps Per SM           warp        48.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       903775
    Memory Throughput                 %        26.34
    DRAM Throughput                   %        26.34
    Duration                         us       602.56
    L1/TEX Cache Throughput           %        11.53
    L2 Cache Throughput               %        32.71
    SM Active Cycles              cycle    898292.57
    Compute (SM) Throughput           %        56.27
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.58
    Max Bandwidth                          %        26.34
    L1/TEX Hit Rate                        %        33.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8438797
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        56.27
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.29
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.71
    Active Warps Per Scheduler          warp        11.68
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.71%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.68 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.74
    Warp Cycles Per Executed Instruction           cycle        20.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.84
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.16%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       902738
    Memory Throughput                 %        26.27
    DRAM Throughput                   %        26.27
    Duration                         us       601.95
    L1/TEX Cache Throughput           %        11.59
    L2 Cache Throughput               %        32.76
    SM Active Cycles              cycle    894124.14
    Compute (SM) Throughput           %        56.38
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.59
    Max Bandwidth                          %        26.27
    L1/TEX Hit Rate                        %        33.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8346899
    L2 Hit Rate                            %        33.48
    Mem Pipes Busy                         %        56.38
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.43
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.57
    Active Warps Per Scheduler          warp        11.67
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.67 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.68
    Warp Cycles Per Executed Instruction           cycle        20.68
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.78
    Achieved Active Warps Per SM           warp        47.22
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       905015
    Memory Throughput                 %        26.21
    DRAM Throughput                   %        26.21
    Duration                         us       603.39
    L1/TEX Cache Throughput           %        11.51
    L2 Cache Throughput               %        32.59
    SM Active Cycles              cycle    897775.07
    Compute (SM) Throughput           %        56.19
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.52
    Max Bandwidth                          %        26.21
    L1/TEX Hit Rate                        %        33.22
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8347014
    L2 Hit Rate                            %        33.57
    Mem Pipes Busy                         %        56.19
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.43
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.57
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.71
    Warp Cycles Per Executed Instruction           cycle        20.72
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.75
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       910773
    Memory Throughput                 %        26.07
    DRAM Throughput                   %        26.07
    Duration                         us       607.23
    L1/TEX Cache Throughput           %        11.40
    L2 Cache Throughput               %        32.51
    SM Active Cycles              cycle    903480.69
    Compute (SM) Throughput           %        55.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.28
    Mem Busy                               %        18.43
    Max Bandwidth                          %        26.07
    L1/TEX Hit Rate                        %        33.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8381757
    L2 Hit Rate                            %        33.66
    Mem Pipes Busy                         %        55.84
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.19
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.81
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.81%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.80
    Warp Cycles Per Executed Instruction           cycle        20.80
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.22%                                                                                          
          On average, each warp of this workload spends 6.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.2% of the total average of 20.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        47.24
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.18%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       905747
    Memory Throughput                 %        26.26
    DRAM Throughput                   %        26.26
    Duration                         us       603.87
    L1/TEX Cache Throughput           %        11.58
    L2 Cache Throughput               %        32.81
    SM Active Cycles              cycle    896807.67
    Compute (SM) Throughput           %        56.15
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.60
    Max Bandwidth                          %        26.26
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8413431
    L2 Hit Rate                            %        33.78
    Mem Pipes Busy                         %        56.15
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.68
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.32
    Active Warps Per Scheduler          warp        11.65
    Eligible Warps Per Scheduler        warp         2.64
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.65 active warps per scheduler, but only an average of 2.64 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.55
    Warp Cycles Per Executed Instruction           cycle        20.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.29
    Achieved Active Warps Per SM           warp        46.91
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.71%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       881159
    Memory Throughput                 %        26.98
    DRAM Throughput                   %        26.98
    Duration                         us       587.58
    L1/TEX Cache Throughput           %        11.85
    L2 Cache Throughput               %        34.36
    SM Active Cycles              cycle    866823.12
    Compute (SM) Throughput           %        57.78
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.04
    Max Bandwidth                          %        26.98
    L1/TEX Hit Rate                        %        33.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8417719
    L2 Hit Rate                            %        33.74
    Mem Pipes Busy                         %        57.78
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.15
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.85
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.68
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.85%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.87 active warps per scheduler, but only an average of 2.68 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.15
    Achieved Active Warps Per SM           warp        47.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.85%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       872012
    Memory Throughput                 %        27.29
    DRAM Throughput                   %        27.29
    Duration                         us       581.34
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.66
    SM Active Cycles              cycle    857200.80
    Compute (SM) Throughput           %        58.32
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.23
    Max Bandwidth                          %        27.29
    L1/TEX Hit Rate                        %        33.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8427874
    L2 Hit Rate                            %        33.68
    Mem Pipes Busy                         %        58.32
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.50
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.50
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         2.78
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.89 active warps per scheduler, but only an average of 2.78 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.99
    Warp Cycles Per Executed Instruction           cycle        19.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.64
    Achieved Active Warps Per SM           warp        47.77
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877297
    Memory Throughput                 %        27.12
    DRAM Throughput                   %        27.12
    Duration                         us       584.90
    L1/TEX Cache Throughput           %        12.00
    L2 Cache Throughput               %        34.36
    SM Active Cycles              cycle    863867.57
    Compute (SM) Throughput           %        57.97
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.10
    Max Bandwidth                          %        27.12
    L1/TEX Hit Rate                        %        32.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8426991
    L2 Hit Rate                            %        33.80
    Mem Pipes Busy                         %        57.97
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.09
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.91
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.25
    Warp Cycles Per Executed Instruction           cycle        20.25
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.10
    Achieved Active Warps Per SM           warp        48.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.9%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874989
    Memory Throughput                 %        27.19
    DRAM Throughput                   %        27.19
    Duration                         us       583.42
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.49
    SM Active Cycles              cycle    857601.07
    Compute (SM) Throughput           %        58.17
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.16
    Max Bandwidth                          %        27.19
    L1/TEX Hit Rate                        %        33.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8422373
    L2 Hit Rate                            %        33.71
    Mem Pipes Busy                         %        58.17
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.57
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.43
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.12
    Warp Cycles Per Executed Instruction           cycle        20.12
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.13
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.87%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877029
    Memory Throughput                 %        27.11
    DRAM Throughput                   %        27.11
    Duration                         us       584.70
    L1/TEX Cache Throughput           %        11.98
    L2 Cache Throughput               %        34.37
    SM Active Cycles              cycle    862688.63
    Compute (SM) Throughput           %        57.98
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.10
    Max Bandwidth                          %        27.11
    L1/TEX Hit Rate                        %        33.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8416705
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        57.98
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.14
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.86
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.86%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.76 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.13
    Achieved Active Warps Per SM           warp        48.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.87%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       872620
    Memory Throughput                 %        27.25
    DRAM Throughput                   %        27.25
    Duration                         us       581.76
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.38
    SM Active Cycles              cycle    856195.34
    Compute (SM) Throughput           %        58.28
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.19
    Max Bandwidth                          %        27.25
    L1/TEX Hit Rate                        %        33.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8401213
    L2 Hit Rate                            %        33.77
    Mem Pipes Busy                         %        58.28
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.69
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.31
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.31%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.84 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.12
    Warp Cycles Per Executed Instruction           cycle        20.12
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.19
    Achieved Active Warps Per SM           warp        48.12
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.81%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.747e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 12 excessive sectors (0% of the total     
          25174614 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       895518
    Memory Throughput                 %        26.59
    DRAM Throughput                   %        26.59
    Duration                         us       597.06
    L1/TEX Cache Throughput           %        11.67
    L2 Cache Throughput               %        32.85
    SM Active Cycles              cycle    890047.33
    Compute (SM) Throughput           %        56.79
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.31
    Mem Busy                               %        18.76
    Max Bandwidth                          %        26.59
    L1/TEX Hit Rate                        %        33.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8432983
    L2 Hit Rate                            %        33.78
    Mem Pipes Busy                         %        56.79
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.75
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.25
    Active Warps Per Scheduler          warp        11.66
    Eligible Warps Per Scheduler        warp         2.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.21%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.66 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.54
    Warp Cycles Per Executed Instruction           cycle        20.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        47.22
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       901297
    Memory Throughput                 %        26.31
    DRAM Throughput                   %        26.31
    Duration                         us       600.90
    L1/TEX Cache Throughput           %        11.54
    L2 Cache Throughput               %        32.63
    SM Active Cycles              cycle    896664.29
    Compute (SM) Throughput           %        56.42
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.58
    Max Bandwidth                          %        26.31
    L1/TEX Hit Rate                        %        33.22
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8347031
    L2 Hit Rate                            %        33.57
    Mem Pipes Busy                         %        56.42
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.50
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.50
    Active Warps Per Scheduler          warp        11.68
    Eligible Warps Per Scheduler        warp         2.57
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.68 active warps per scheduler, but only an average of 2.57 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.67
    Warp Cycles Per Executed Instruction           cycle        20.68
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        47.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.14%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       900066
    Memory Throughput                 %        26.35
    DRAM Throughput                   %        26.35
    Duration                         us       600.10
    L1/TEX Cache Throughput           %        11.61
    L2 Cache Throughput               %        32.54
    SM Active Cycles              cycle    892853.45
    Compute (SM) Throughput           %        56.50
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.63
    Max Bandwidth                          %        26.35
    L1/TEX Hit Rate                        %        33.22
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8349210
    L2 Hit Rate                            %        33.63
    Mem Pipes Busy                         %        56.50
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.59
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.41
    Active Warps Per Scheduler          warp        11.68
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.68 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.65
    Warp Cycles Per Executed Instruction           cycle        20.65
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        47.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       904713
    Memory Throughput                 %        26.21
    DRAM Throughput                   %        26.21
    Duration                         us       603.26
    L1/TEX Cache Throughput           %        11.54
    L2 Cache Throughput               %        32.75
    SM Active Cycles              cycle    895744.06
    Compute (SM) Throughput           %        56.27
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.52
    Max Bandwidth                          %        26.21
    L1/TEX Hit Rate                        %        33.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8344457
    L2 Hit Rate                            %        33.64
    Mem Pipes Busy                         %        56.27
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.61
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.39
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.61
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.61 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.66
    Warp Cycles Per Executed Instruction           cycle        20.66
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.80
    Achieved Active Warps Per SM           warp        47.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.2%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       910820
    Memory Throughput                 %        26.07
    DRAM Throughput                   %        26.07
    Duration                         us       607.33
    L1/TEX Cache Throughput           %        11.38
    L2 Cache Throughput               %        32.60
    SM Active Cycles              cycle    904867.55
    Compute (SM) Throughput           %        55.89
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.28
    Mem Busy                               %        18.44
    Max Bandwidth                          %        26.07
    L1/TEX Hit Rate                        %        33.32
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8384788
    L2 Hit Rate                            %        33.65
    Mem Pipes Busy                         %        55.89
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.34
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.66
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.75
    Warp Cycles Per Executed Instruction           cycle        20.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.52%                                                                                          
          On average, each warp of this workload spends 6.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.5% of the total average of 20.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.90
    Achieved Active Warps Per SM           warp        47.30
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.1%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       905291
    Memory Throughput                 %        26.27
    DRAM Throughput                   %        26.27
    Duration                         us       603.52
    L1/TEX Cache Throughput           %        11.59
    L2 Cache Throughput               %        32.68
    SM Active Cycles              cycle    896061.31
    Compute (SM) Throughput           %        56.17
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.54
    Max Bandwidth                          %        26.27
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8412597
    L2 Hit Rate                            %        33.82
    Mem Pipes Busy                         %        56.17
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.71
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.29
    Active Warps Per Scheduler          warp        11.66
    Eligible Warps Per Scheduler        warp         2.64
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.29%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.66 active warps per scheduler, but only an average of 2.64 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.56
    Warp Cycles Per Executed Instruction           cycle        20.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.29
    Achieved Active Warps Per SM           warp        46.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.71%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878920
    Memory Throughput                 %        27.06
    DRAM Throughput                   %        27.06
    Duration                         us       585.98
    L1/TEX Cache Throughput           %        11.86
    L2 Cache Throughput               %        34.46
    SM Active Cycles              cycle    865291.45
    Compute (SM) Throughput           %        57.86
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.10
    Max Bandwidth                          %        27.06
    L1/TEX Hit Rate                        %        33.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8413817
    L2 Hit Rate                            %        33.73
    Mem Pipes Busy                         %        57.86
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.09
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.91
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.68
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.87 active warps per scheduler, but only an average of 2.68 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.10
    Warp Cycles Per Executed Instruction           cycle        20.10
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.22
    Achieved Active Warps Per SM           warp        47.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       871101
    Memory Throughput                 %        27.31
    DRAM Throughput                   %        27.31
    Duration                         us       580.77
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        34.67
    SM Active Cycles              cycle    856751.36
    Compute (SM) Throughput           %        58.38
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.20
    Max Bandwidth                          %        27.31
    L1/TEX Hit Rate                        %        33.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8439154
    L2 Hit Rate                            %        33.93
    Mem Pipes Busy                         %        58.38
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.53
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.47
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         2.78
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.90 active warps per scheduler, but only an average of 2.78 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.99
    Warp Cycles Per Executed Instruction           cycle        19.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.68
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877658
    Memory Throughput                 %        27.10
    DRAM Throughput                   %        27.10
    Duration                         us       585.18
    L1/TEX Cache Throughput           %        12.02
    L2 Cache Throughput               %        34.35
    SM Active Cycles              cycle    862294.55
    Compute (SM) Throughput           %        57.99
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.09
    Max Bandwidth                          %        27.10
    L1/TEX Hit Rate                        %        32.52
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8417611
    L2 Hit Rate                            %        33.72
    Mem Pipes Busy                         %        57.99
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.09
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.91
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.97 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.25
    Warp Cycles Per Executed Instruction           cycle        20.25
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.10
    Achieved Active Warps Per SM           warp        48.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.9%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874050
    Memory Throughput                 %        27.20
    DRAM Throughput                   %        27.20
    Duration                         us       582.72
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.57
    SM Active Cycles              cycle    856846.38
    Compute (SM) Throughput           %        58.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.15
    Max Bandwidth                          %        27.20
    L1/TEX Hit Rate                        %        33.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8407935
    L2 Hit Rate                            %        33.64
    Mem Pipes Busy                         %        58.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.66
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.34
    Active Warps Per Scheduler          warp        12.00
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.34%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.00 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.11
    Warp Cycles Per Executed Instruction           cycle        20.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.13
    Achieved Active Warps Per SM           warp        48.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.87%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877129
    Memory Throughput                 %        27.10
    DRAM Throughput                   %        27.10
    Duration                         us       584.77
    L1/TEX Cache Throughput           %        11.96
    L2 Cache Throughput               %        34.40
    SM Active Cycles              cycle    862303.84
    Compute (SM) Throughput           %        57.98
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        19.09
    Max Bandwidth                          %        27.10
    L1/TEX Hit Rate                        %        33.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8408824
    L2 Hit Rate                            %        33.76
    Mem Pipes Busy                         %        57.98
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.18
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.82
    Active Warps Per Scheduler          warp        11.98
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.82%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.98 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.24
    Warp Cycles Per Executed Instruction           cycle        20.24
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.08
    Achieved Active Warps Per SM           warp        48.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.92%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.58e-05%                                                                                       
          This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (0% of the total      
          25184030 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       872082
    Memory Throughput                 %        27.27
    DRAM Throughput                   %        27.27
    Duration                         us       581.50
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        34.59
    SM Active Cycles              cycle    855076.09
    Compute (SM) Throughput           %        58.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.34
    Mem Busy                               %        19.15
    Max Bandwidth                          %        27.27
    L1/TEX Hit Rate                        %        33.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8403277
    L2 Hit Rate                            %        33.81
    Mem Pipes Busy                         %        58.36
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.78
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.22
    Active Warps Per Scheduler          warp        12.00
    Eligible Warps Per Scheduler        warp         2.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.22%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.00 active warps per scheduler, but only an average of 2.85 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.08
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.97
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.20
    Achieved Active Warps Per SM           warp        48.13
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.8%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 3.952e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 10 excessive sectors (0% of the total     
          25178838 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       903973
    Memory Throughput                 %        28.51
    DRAM Throughput                   %        28.51
    Duration                         us       602.69
    L1/TEX Cache Throughput           %        11.63
    L2 Cache Throughput               %        34.55
    SM Active Cycles              cycle    898371.03
    Compute (SM) Throughput           %        56.26
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.40
    Mem Busy                               %        19.91
    Max Bandwidth                          %        28.51
    L1/TEX Hit Rate                        %        38.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10444407
    L2 Hit Rate                            %        38.72
    Mem Pipes Busy                         %        56.26
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.18
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.82
    Active Warps Per Scheduler          warp        11.66
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.74%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.66 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.75
    Warp Cycles Per Executed Instruction           cycle        20.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.68
    Achieved Active Warps Per SM           warp        47.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       909082
    Memory Throughput                 %        28.44
    DRAM Throughput                   %        28.44
    Duration                         us       606.08
    L1/TEX Cache Throughput           %        11.64
    L2 Cache Throughput               %        34.47
    SM Active Cycles              cycle    901447.67
    Compute (SM) Throughput           %        55.94
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.40
    Mem Busy                               %        19.83
    Max Bandwidth                          %        28.44
    L1/TEX Hit Rate                        %        38.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10530623
    L2 Hit Rate                            %        38.79
    Mem Pipes Busy                         %        55.94
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.03
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.97
    Active Warps Per Scheduler          warp        11.62
    Eligible Warps Per Scheduler        warp         2.55
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.62 active warps per scheduler, but only an average of 2.55 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.74
    Warp Cycles Per Executed Instruction           cycle        20.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.54
    Achieved Active Warps Per SM           warp        47.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.46%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       913175
    Memory Throughput                 %        28.29
    DRAM Throughput                   %        28.29
    Duration                         us       608.93
    L1/TEX Cache Throughput           %        11.53
    L2 Cache Throughput               %        34.42
    SM Active Cycles              cycle    907357.30
    Compute (SM) Throughput           %        55.74
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.39
    Mem Busy                               %        19.73
    Max Bandwidth                          %        28.29
    L1/TEX Hit Rate                        %        38.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10524168
    L2 Hit Rate                            %        38.90
    Mem Pipes Busy                         %        55.74
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.64
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.36
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.53
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.53 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.92
    Warp Cycles Per Executed Instruction           cycle        20.92
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.51
    Achieved Active Warps Per SM           warp        47.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.49%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       913937
    Memory Throughput                 %        28.27
    DRAM Throughput                   %        28.27
    Duration                         us       609.34
    L1/TEX Cache Throughput           %        11.55
    L2 Cache Throughput               %        34.42
    SM Active Cycles              cycle    906894.99
    Compute (SM) Throughput           %        55.65
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.39
    Mem Busy                               %        19.76
    Max Bandwidth                          %        28.27
    L1/TEX Hit Rate                        %        38.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10519741
    L2 Hit Rate                            %        38.90
    Mem Pipes Busy                         %        55.65
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.84
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.16
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.54
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.16%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.54 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.84
    Warp Cycles Per Executed Instruction           cycle        20.84
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.46
    Achieved Active Warps Per SM           warp        47.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.54%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       916914
    Memory Throughput                 %        28.18
    DRAM Throughput                   %        28.18
    Duration                         us       611.30
    L1/TEX Cache Throughput           %        11.50
    L2 Cache Throughput               %        34.31
    SM Active Cycles              cycle    908985.26
    Compute (SM) Throughput           %        55.46
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.38
    Mem Busy                               %        19.68
    Max Bandwidth                          %        28.18
    L1/TEX Hit Rate                        %        38.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10512279
    L2 Hit Rate                            %        38.83
    Mem Pipes Busy                         %        55.46
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.70
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.30
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.55
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.3%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.55 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.90
    Warp Cycles Per Executed Instruction           cycle        20.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.08%                                                                                          
          On average, each warp of this workload spends 6.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.1% of the total average of 20.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.53
    Achieved Active Warps Per SM           warp        47.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       922433
    Memory Throughput                 %        27.94
    DRAM Throughput                   %        27.94
    Duration                         us       615.07
    L1/TEX Cache Throughput           %        11.38
    L2 Cache Throughput               %        34.12
    SM Active Cycles              cycle    914028.52
    Compute (SM) Throughput           %        55.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.37
    Mem Busy                               %        19.54
    Max Bandwidth                          %        27.94
    L1/TEX Hit Rate                        %        38.40
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10460995
    L2 Hit Rate                            %        38.72
    Mem Pipes Busy                         %        55.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.53
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.47
    Active Warps Per Scheduler          warp        11.67
    Eligible Warps Per Scheduler        warp         2.55
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.67 active warps per scheduler, but only an average of 2.55 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.01
    Warp Cycles Per Executed Instruction           cycle        21.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.79%                                                                                          
          On average, each warp of this workload spends 6.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.8% of the total average of 21.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.50
    Achieved Active Warps Per SM           warp        47.04
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       914411
    Memory Throughput                 %        28.13
    DRAM Throughput                   %        28.13
    Duration                         us       609.63
    L1/TEX Cache Throughput           %        11.59
    L2 Cache Throughput               %        34.45
    SM Active Cycles              cycle    904471.66
    Compute (SM) Throughput           %        55.62
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.38
    Mem Busy                               %        19.70
    Max Bandwidth                          %        28.13
    L1/TEX Hit Rate                        %        38.26
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10397505
    L2 Hit Rate                            %        38.54
    Mem Pipes Busy                         %        55.62
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.11
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.89
    Active Warps Per Scheduler          warp        11.62
    Eligible Warps Per Scheduler        warp         2.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.62 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.71
    Warp Cycles Per Executed Instruction           cycle        20.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.10
    Achieved Active Warps Per SM           warp        46.78
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.9%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       882009
    Memory Throughput                 %        29.08
    DRAM Throughput                   %        29.08
    Duration                         us       588.03
    L1/TEX Cache Throughput           %        11.92
    L2 Cache Throughput               %        36.33
    SM Active Cycles              cycle    870376.70
    Compute (SM) Throughput           %        57.66
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.43
    Mem Busy                               %        20.31
    Max Bandwidth                          %        29.08
    L1/TEX Hit Rate                        %        38.10
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10327325
    L2 Hit Rate                            %        38.38
    Mem Pipes Busy                         %        57.66
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.80
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.20
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.2%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.82 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.10
    Warp Cycles Per Executed Instruction           cycle        20.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.94
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.06%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874477
    Memory Throughput                 %        29.17
    DRAM Throughput                   %        29.17
    Duration                         us       583.01
    L1/TEX Cache Throughput           %        12.18
    L2 Cache Throughput               %        35.92
    SM Active Cycles              cycle    860960.27
    Compute (SM) Throughput           %        58.15
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.43
    Mem Busy                               %        20.37
    Max Bandwidth                          %        29.17
    L1/TEX Hit Rate                        %        37.76
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10179609
    L2 Hit Rate                            %        37.96
    Mem Pipes Busy                         %        58.15
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.22
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.78
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.78%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.87 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.05
    Warp Cycles Per Executed Instruction           cycle        20.05
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.02
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.47
    Achieved Active Warps Per SM           warp        47.66
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.53%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       880937
    Memory Throughput                 %        28.65
    DRAM Throughput                   %        28.65
    Duration                         us       587.33
    L1/TEX Cache Throughput           %        12.05
    L2 Cache Throughput               %        35.70
    SM Active Cycles              cycle    867395.99
    Compute (SM) Throughput           %        57.73
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.41
    Mem Busy                               %        20.02
    Max Bandwidth                          %        28.65
    L1/TEX Hit Rate                        %        36.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      9909693
    L2 Hit Rate                            %        37.39
    Mem Pipes Busy                         %        57.73
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.80
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.20
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         2.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.2%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.93 active warps per scheduler, but only an average of 2.72 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.28
    Warp Cycles Per Executed Instruction           cycle        20.28
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.01
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.82
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.18%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877146
    Memory Throughput                 %        28.27
    DRAM Throughput                   %        28.27
    Duration                         us       584.83
    L1/TEX Cache Throughput           %        12.11
    L2 Cache Throughput               %        35.36
    SM Active Cycles              cycle    860164.92
    Compute (SM) Throughput           %        58.02
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.39
    Mem Busy                               %        19.81
    Max Bandwidth                          %        28.27
    L1/TEX Hit Rate                        %        36.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      9459824
    L2 Hit Rate                            %        36.31
    Mem Pipes Busy                         %        58.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.29
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.71
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         2.80
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.71%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.95 active warps per scheduler, but only an average of 2.80 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.00
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.85
    Achieved Active Warps Per SM           warp        47.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.15%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 7.58e-06%                                                                                       
          This kernel has uncoalesced global accesses resulting in a total of 2 excessive sectors (0% of the total      
          26235394 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       878137
    Memory Throughput                 %        27.63
    DRAM Throughput                   %        27.63
    Duration                         us       585.47
    L1/TEX Cache Throughput           %        11.95
    L2 Cache Throughput               %        34.78
    SM Active Cycles              cycle    863150.43
    Compute (SM) Throughput           %        57.91
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.41
    Max Bandwidth                          %        27.63
    L1/TEX Hit Rate                        %        34.68
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8910650
    L2 Hit Rate                            %        34.99
    Mem Pipes Busy                         %        57.91
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.14
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.86
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.77
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.86%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.77 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.23
    Warp Cycles Per Executed Instruction           cycle        20.23
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.98
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.98
    Achieved Active Warps Per SM           warp        47.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.02%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.551e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (0% of the total      
          25686194 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       870727
    Memory Throughput                 %        27.57
    DRAM Throughput                   %        27.57
    Duration                         us       580.51
    L1/TEX Cache Throughput           %        12.12
    L2 Cache Throughput               %        34.80
    SM Active Cycles              cycle    852964.92
    Compute (SM) Throughput           %        58.40
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.35
    Mem Busy                               %        19.36
    Max Bandwidth                          %        27.57
    L1/TEX Hit Rate                        %        34.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8643370
    L2 Hit Rate                            %        34.44
    Mem Pipes Busy                         %        58.40
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.87
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.13
    Active Warps Per Scheduler          warp        12.01
    Eligible Warps Per Scheduler        warp         2.86
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 12.01 active warps per scheduler, but only an average of 2.86 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.06
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.98
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.24
    Achieved Active Warps Per SM           warp        48.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.76%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.349e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 6 excessive sectors (0% of the total      
          25419004 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       918984
    Memory Throughput                 %        30.31
    DRAM Throughput                   %        30.31
    Duration                         us       612.70
    L1/TEX Cache Throughput           %        13.00
    L2 Cache Throughput               %        36.14
    SM Active Cycles              cycle    914080.42
    Compute (SM) Throughput           %        55.34
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.49
    Mem Busy                               %        20.92
    Max Bandwidth                          %        30.31
    L1/TEX Hit Rate                        %        42.85
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12585052
    L2 Hit Rate                            %        43.22
    Mem Pipes Busy                         %        55.34
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.37
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.63
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         2.50
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.63%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.64 active warps per scheduler, but only an average of 2.50 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.02
    Warp Cycles Per Executed Instruction           cycle        21.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.25%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.2% of the total average of 21.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.44
    Achieved Active Warps Per SM           warp        47.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.56%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       917218
    Memory Throughput                 %        30.38
    DRAM Throughput                   %        30.38
    Duration                         us       611.52
    L1/TEX Cache Throughput           %        13.02
    L2 Cache Throughput               %        36.38
    SM Active Cycles              cycle    912821.33
    Compute (SM) Throughput           %        55.44
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.49
    Mem Busy                               %        20.96
    Max Bandwidth                          %        30.38
    L1/TEX Hit Rate                        %        42.85
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12583863
    L2 Hit Rate                            %        43.33
    Mem Pipes Busy                         %        55.44
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.44
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.56
    Active Warps Per Scheduler          warp        11.60
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.60 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.91
    Warp Cycles Per Executed Instruction           cycle        20.92
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.09%                                                                                          
          On average, each warp of this workload spends 6.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.1% of the total average of 20.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.36
    Achieved Active Warps Per SM           warp        46.95
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.64%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       919035
    Memory Throughput                 %        30.31
    DRAM Throughput                   %        30.31
    Duration                         us       612.83
    L1/TEX Cache Throughput           %        13.02
    L2 Cache Throughput               %        36.14
    SM Active Cycles              cycle    912570.47
    Compute (SM) Throughput           %        55.39
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.49
    Mem Busy                               %        20.93
    Max Bandwidth                          %        30.31
    L1/TEX Hit Rate                        %        42.85
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12583168
    L2 Hit Rate                            %        43.22
    Mem Pipes Busy                         %        55.39
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.41
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.59
    Active Warps Per Scheduler          warp        11.58
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.58 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.90
    Warp Cycles Per Executed Instruction           cycle        20.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        46.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.72%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       923150
    Memory Throughput                 %        30.15
    DRAM Throughput                   %        30.15
    Duration                         us       615.49
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %        35.98
    SM Active Cycles              cycle    918703.43
    Compute (SM) Throughput           %        55.09
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.48
    Mem Busy                               %        20.82
    Max Bandwidth                          %        30.15
    L1/TEX Hit Rate                        %        42.79
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12553895
    L2 Hit Rate                            %        43.18
    Mem Pipes Busy                         %        55.09
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        54.99
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        45.01
    Active Warps Per Scheduler          warp        11.61
    Eligible Warps Per Scheduler        warp         2.48
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.91%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.61 active warps per scheduler, but only an average of 2.48 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.11
    Warp Cycles Per Executed Instruction           cycle        21.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.58%                                                                                          
          On average, each warp of this workload spends 6.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.6% of the total average of 21.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        46.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.68%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       922059
    Memory Throughput                 %        30.19
    DRAM Throughput                   %        30.19
    Duration                         us       614.82
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %        36.21
    SM Active Cycles              cycle    917254.58
    Compute (SM) Throughput           %        55.20
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.48
    Mem Busy                               %        20.90
    Max Bandwidth                          %        30.19
    L1/TEX Hit Rate                        %        42.78
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12550691
    L2 Hit Rate                            %        43.08
    Mem Pipes Busy                         %        55.20
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.29
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.71
    Active Warps Per Scheduler          warp        11.60
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.71%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.60 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.98
    Warp Cycles Per Executed Instruction           cycle        20.98
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.61%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.6% of the total average of 21.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.31
    Achieved Active Warps Per SM           warp        46.92
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.69%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       927453
    Memory Throughput                 %        29.98
    DRAM Throughput                   %        29.98
    Duration                         us       618.34
    L1/TEX Cache Throughput           %        12.89
    L2 Cache Throughput               %        35.95
    SM Active Cycles              cycle    917955.23
    Compute (SM) Throughput           %        54.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.47
    Mem Busy                               %        20.79
    Max Bandwidth                          %        29.98
    L1/TEX Hit Rate                        %        42.72
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12516291
    L2 Hit Rate                            %        43.02
    Mem Pipes Busy                         %        54.83
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.08
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.92
    Active Warps Per Scheduler          warp        11.62
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.62 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.09
    Warp Cycles Per Executed Instruction           cycle        21.10
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.55%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.6% of the total average of 21.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.27
    Achieved Active Warps Per SM           warp        46.89
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       932495
    Memory Throughput                 %        29.75
    DRAM Throughput                   %        29.75
    Duration                         us       621.66
    L1/TEX Cache Throughput           %        12.73
    L2 Cache Throughput               %        35.80
    SM Active Cycles              cycle    924672.67
    Compute (SM) Throughput           %        54.54
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.46
    Mem Busy                               %        20.62
    Max Bandwidth                          %        29.75
    L1/TEX Hit Rate                        %        42.59
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12451185
    L2 Hit Rate                            %        42.92
    Mem Pipes Busy                         %        54.54
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.12
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.88
    Active Warps Per Scheduler          warp        11.62
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.88%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.62 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.09
    Warp Cycles Per Executed Instruction           cycle        21.09
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 31.79%                                                                                          
          On average, each warp of this workload spends 6.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 21.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.37
    Achieved Active Warps Per SM           warp        46.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.63%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       924107
    Memory Throughput                 %        29.88
    DRAM Throughput                   %        29.88
    Duration                         us       616.13
    L1/TEX Cache Throughput           %        12.76
    L2 Cache Throughput               %        36.15
    SM Active Cycles              cycle    914192.37
    Compute (SM) Throughput           %        55.03
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.47
    Mem Busy                               %        20.72
    Max Bandwidth                          %        29.88
    L1/TEX Hit Rate                        %        42.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12327722
    L2 Hit Rate                            %        42.80
    Mem Pipes Busy                         %        55.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.76
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.24
    Active Warps Per Scheduler          warp        11.61
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.24%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.61 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.82
    Warp Cycles Per Executed Instruction           cycle        20.82
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.09
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        46.70
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.03%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       887158
    Memory Throughput                 %        30.82
    DRAM Throughput                   %        30.82
    Duration                         us       591.55
    L1/TEX Cache Throughput           %        13.06
    L2 Cache Throughput               %        38.03
    SM Active Cycles              cycle    876301.70
    Compute (SM) Throughput           %        57.38
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.51
    Mem Busy                               %        21.34
    Max Bandwidth                          %        30.82
    L1/TEX Hit Rate                        %        41.79
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     12048465
    L2 Hit Rate                            %        42.13
    Mem Pipes Busy                         %        57.38
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.44
    Issued Warp Per Scheduler                        0.58
    No Eligible                            %        41.56
    Active Warps Per Scheduler          warp        11.77
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.77 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.15
    Warp Cycles Per Executed Instruction           cycle        20.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.08
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.11%                                                                                          
          On average, each warp of this workload spends 6.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.1% of the total average of 20.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.71
    Achieved Active Warps Per SM           warp        47.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       877253
    Memory Throughput                 %        30.58
    DRAM Throughput                   %        30.58
    Duration                         us       584.86
    L1/TEX Cache Throughput           %        12.78
    L2 Cache Throughput               %        37.66
    SM Active Cycles              cycle    865083.28
    Compute (SM) Throughput           %        57.97
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.50
    Mem Busy                               %        21.20
    Max Bandwidth                          %        30.58
    L1/TEX Hit Rate                        %        40.69
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     11510480
    L2 Hit Rate                            %        40.96
    Mem Pipes Busy                         %        57.97
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.97
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.03
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         2.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.03%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.83 active warps per scheduler, but only an average of 2.72 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.06
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.24
    Achieved Active Warps Per SM           warp        47.52
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.76%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       882226
    Memory Throughput                 %        29.23
    DRAM Throughput                   %        29.23
    Duration                         us       588.16
    L1/TEX Cache Throughput           %        12.03
    L2 Cache Throughput               %        36.32
    SM Active Cycles              cycle    869110.33
    Compute (SM) Throughput           %        57.64
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.44
    Mem Busy                               %        20.35
    Max Bandwidth                          %        29.23
    L1/TEX Hit Rate                        %        37.09
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10439471
    L2 Hit Rate                            %        38.75
    Mem Pipes Busy                         %        57.64
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.70
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.30
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         2.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.3%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.89 active warps per scheduler, but only an average of 2.71 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.26
    Warp Cycles Per Executed Instruction           cycle        20.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.03
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.65
    Achieved Active Warps Per SM           warp        47.78
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.35%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       875081
    Memory Throughput                 %        28.32
    DRAM Throughput                   %        28.32
    Duration                         us       583.42
    L1/TEX Cache Throughput           %        12.10
    L2 Cache Throughput               %        35.63
    SM Active Cycles              cycle    858475.61
    Compute (SM) Throughput           %        58.11
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.39
    Mem Busy                               %        19.86
    Max Bandwidth                          %        28.32
    L1/TEX Hit Rate                        %        35.95
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      9418015
    L2 Hit Rate                            %        36.32
    Mem Pipes Busy                         %        58.11
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.51
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.49
    Active Warps Per Scheduler          warp        11.94
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.94 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.07
    Warp Cycles Per Executed Instruction           cycle        20.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.00
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.81
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.19%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 7.597e-06%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 2 excessive sectors (0% of the total      
          26193666 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       875092
    Memory Throughput                 %        27.73
    DRAM Throughput                   %        27.73
    Duration                         us       583.42
    L1/TEX Cache Throughput           %        11.98
    L2 Cache Throughput               %        35.07
    SM Active Cycles              cycle    859601.56
    Compute (SM) Throughput           %        58.11
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.47
    Max Bandwidth                          %        27.73
    L1/TEX Hit Rate                        %        34.67
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8904075
    L2 Hit Rate                            %        35.10
    Mem Pipes Busy                         %        58.11
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.32
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.68
    Active Warps Per Scheduler          warp        11.99
    Eligible Warps Per Scheduler        warp         2.80
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.68%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.99 active warps per scheduler, but only an average of 2.80 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.22
    Warp Cycles Per Executed Instruction           cycle        20.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.98
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.25
    Achieved Active Warps Per SM           warp        48.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       867350
    Memory Throughput                 %        27.70
    DRAM Throughput                   %        27.70
    Duration                         us       578.34
    L1/TEX Cache Throughput           %        12.13
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle    850194.96
    Compute (SM) Throughput           %        58.68
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.36
    Mem Busy                               %        19.49
    Max Bandwidth                          %        27.70
    L1/TEX Hit Rate                        %        34.01
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      8650342
    L2 Hit Rate                            %        34.39
    Mem Pipes Busy                         %        58.68
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.11
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        39.89
    Active Warps Per Scheduler          warp        12.04
    Eligible Warps Per Scheduler        warp         2.89
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.03
    Warp Cycles Per Executed Instruction           cycle        20.03
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.98
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.45
    Achieved Active Warps Per SM           warp        48.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       876854
    Memory Throughput                 %        23.93
    DRAM Throughput                   %        23.93
    Duration                         us       584.61
    L1/TEX Cache Throughput           %        11.58
    L2 Cache Throughput               %        30.69
    SM Active Cycles              cycle    871652.07
    Compute (SM) Throughput           %        58.00
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.18
    Mem Busy                               %        17.15
    Max Bandwidth                          %        23.93
    L1/TEX Hit Rate                        %        24.74
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      5518586
    L2 Hit Rate                            %        25.24
    Mem Pipes Busy                         %        58.00
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.00
    Issued Warp Per Scheduler                        0.58
    No Eligible                            %        42.00
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.68
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 42%                                                                                       
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.81 active warps per scheduler, but only an average of 2.68 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.36
    Warp Cycles Per Executed Instruction           cycle        20.36
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.88
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.75
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       910282
    Memory Throughput                 %        28.88
    DRAM Throughput                   %        28.88
    Duration                         us       606.88
    L1/TEX Cache Throughput           %        11.80
    L2 Cache Throughput               %        34.91
    SM Active Cycles              cycle    903750.99
    Compute (SM) Throughput           %        55.86
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.42
    Mem Busy                               %        20.09
    Max Bandwidth                          %        28.88
    L1/TEX Hit Rate                        %        39.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     10978727
    L2 Hit Rate                            %        39.92
    Mem Pipes Busy                         %        55.86
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.03
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        43.97
    Active Warps Per Scheduler          warp        11.65
    Eligible Warps Per Scheduler        warp         2.54
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.65 active warps per scheduler, but only an average of 2.54 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.80
    Warp Cycles Per Executed Instruction           cycle        20.80
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.05
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.71
    Achieved Active Warps Per SM           warp        47.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       890780
    Memory Throughput                 %        26.18
    DRAM Throughput                   %        26.18
    Duration                         us       593.95
    L1/TEX Cache Throughput           %        11.52
    L2 Cache Throughput               %        31.66
    SM Active Cycles              cycle    885332.39
    Compute (SM) Throughput           %        57.14
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.29
    Mem Busy                               %        18.10
    Max Bandwidth                          %        26.18
    L1/TEX Hit Rate                        %        29.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      7002483
    L2 Hit Rate                            %        30.00
    Mem Pipes Busy                         %        57.14
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        57.19
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        42.81
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 42.81%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.63 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.45
    Warp Cycles Per Executed Instruction           cycle        20.45
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.93
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.00
    Achieved Active Warps Per SM           warp        47.36
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26%                                                                                             
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       890290
    Memory Throughput                 %        26.11
    DRAM Throughput                   %        26.11
    Duration                         us       593.66
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %        31.80
    SM Active Cycles              cycle    886247.03
    Compute (SM) Throughput           %        57.17
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.28
    Mem Busy                               %        18.09
    Max Bandwidth                          %        26.11
    L1/TEX Hit Rate                        %        29.25
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      6939940
    L2 Hit Rate                            %        29.77
    Mem Pipes Busy                         %        57.17
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        57.08
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        42.92
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 42.83%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.48
    Warp Cycles Per Executed Instruction           cycle        20.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.92
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.88
    Achieved Active Warps Per SM           warp        47.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.12%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       895756
    Memory Throughput                 %        25.82
    DRAM Throughput                   %        25.82
    Duration                         us       597.22
    L1/TEX Cache Throughput           %        11.50
    L2 Cache Throughput               %        31.56
    SM Active Cycles              cycle    891593.11
    Compute (SM) Throughput           %        56.77
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.27
    Mem Busy                               %        17.93
    Max Bandwidth                          %        25.82
    L1/TEX Hit Rate                        %        28.87
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      6811258
    L2 Hit Rate                            %        29.29
    Mem Pipes Busy                         %        56.77
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.68
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.32
    Active Warps Per Scheduler          warp        11.70
    Eligible Warps Per Scheduler        warp         2.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.23%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.70 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.65
    Warp Cycles Per Executed Instruction           cycle        20.65
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.92
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.91
    Achieved Active Warps Per SM           warp        47.30
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.09%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       894817
    Memory Throughput                 %        25.56
    DRAM Throughput                   %        25.56
    Duration                         us       596.58
    L1/TEX Cache Throughput           %        11.52
    L2 Cache Throughput               %        31.39
    SM Active Cycles              cycle    889200.89
    Compute (SM) Throughput           %        56.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.26
    Mem Busy                               %        17.77
    Max Bandwidth                          %        25.56
    L1/TEX Hit Rate                        %        28.08
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      6556974
    L2 Hit Rate                            %        28.60
    Mem Pipes Busy                         %        56.83
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.90
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.10
    Active Warps Per Scheduler          warp        11.70
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.70 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.56
    Warp Cycles Per Executed Instruction           cycle        20.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.91
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.93
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.07%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       898026
    Memory Throughput                 %        25.00
    DRAM Throughput                   %        25.00
    Duration                         us       598.72
    L1/TEX Cache Throughput           %        11.47
    L2 Cache Throughput               %        30.90
    SM Active Cycles              cycle    889067.52
    Compute (SM) Throughput           %        56.63
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.23
    Mem Busy                               %        17.43
    Max Bandwidth                          %        25.00
    L1/TEX Hit Rate                        %        26.68
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      6107600
    L2 Hit Rate                            %        27.14
    Mem Pipes Busy                         %        56.63
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        57.00
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.00
    Active Warps Per Scheduler          warp        11.73
    Eligible Warps Per Scheduler        warp         2.64
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43%                                                                                       
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.73 active warps per scheduler, but only an average of 2.64 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.58
    Warp Cycles Per Executed Instruction           cycle        20.58
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.90
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.99
    Achieved Active Warps Per SM           warp        47.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.01%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       902757
    Memory Throughput                 %        24.85
    DRAM Throughput                   %        24.85
    Duration                         us       601.86
    L1/TEX Cache Throughput           %        11.37
    L2 Cache Throughput               %        30.61
    SM Active Cycles              cycle    894118.28
    Compute (SM) Throughput           %        56.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.22
    Mem Busy                               %        17.34
    Max Bandwidth                          %        24.85
    L1/TEX Hit Rate                        %        26.66
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      6100783
    L2 Hit Rate                            %        27.22
    Mem Pipes Busy                         %        56.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.74
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.26
    Active Warps Per Scheduler          warp        11.75
    Eligible Warps Per Scheduler        warp         2.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 43.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.75 active warps per scheduler, but only an average of 2.63 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.70
    Warp Cycles Per Executed Instruction           cycle        20.70
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.90
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.00
    Achieved Active Warps Per SM           warp        47.36
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26%                                                                                             
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       896455
    Memory Throughput                 %        24.63
    DRAM Throughput                   %        24.63
    Duration                         us       597.66
    L1/TEX Cache Throughput           %        11.50
    L2 Cache Throughput               %        30.53
    SM Active Cycles              cycle    884749.65
    Compute (SM) Throughput           %        56.73
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.21
    Mem Busy                               %        17.21
    Max Bandwidth                          %        24.63
    L1/TEX Hit Rate                        %        25.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      5735350
    L2 Hit Rate                            %        26.07
    Mem Pipes Busy                         %        56.73
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        57.19
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        42.81
    Active Warps Per Scheduler          warp        11.69
    Eligible Warps Per Scheduler        warp         2.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 42.81%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.69 active warps per scheduler, but only an average of 2.67 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.45
    Warp Cycles Per Executed Instruction           cycle        20.45
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.89
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.66
    Achieved Active Warps Per SM           warp        47.14
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.34%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       874405
    Memory Throughput                 %        24.67
    DRAM Throughput                   %        24.67
    Duration                         us       582.94
    L1/TEX Cache Throughput           %        11.71
    L2 Cache Throughput               %        31.27
    SM Active Cycles              cycle    858098.95
    Compute (SM) Throughput           %        58.16
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.21
    Mem Busy                               %        17.27
    Max Bandwidth                          %        24.67
    L1/TEX Hit Rate                        %        23.73
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      5253191
    L2 Hit Rate                            %        24.28
    Mem Pipes Busy                         %        58.16
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.65
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        40.35
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         2.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 40.35%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.96 active warps per scheduler, but only an average of 2.76 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.05
    Warp Cycles Per Executed Instruction           cycle        20.05
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.87
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.68
    Achieved Active Warps Per SM           warp        47.79
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       864293
    Memory Throughput                 %        23.71
    DRAM Throughput                   %        23.71
    Duration                         us       576.19
    L1/TEX Cache Throughput           %        11.83
    L2 Cache Throughput               %        30.38
    SM Active Cycles              cycle    848752.83
    Compute (SM) Throughput           %        58.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.17
    Mem Busy                               %        16.68
    Max Bandwidth                          %        23.71
    L1/TEX Hit Rate                        %        19.64
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      4101438
    L2 Hit Rate                            %        20.15
    Mem Pipes Busy                         %        58.83
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.09
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        39.91
    Active Warps Per Scheduler          warp        11.94
    Eligible Warps Per Scheduler        warp         2.86
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.87
    Warp Cycles Per Executed Instruction           cycle        19.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.84
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.94
    Achieved Active Warps Per SM           warp        47.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       866118
    Memory Throughput                 %        21.33
    DRAM Throughput                   %        21.33
    Duration                         us       577.44
    L1/TEX Cache Throughput           %        11.58
    L2 Cache Throughput               %        28.27
    SM Active Cycles              cycle    850140.55
    Compute (SM) Throughput           %        58.71
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.05
    Mem Busy                               %        15.23
    Max Bandwidth                          %        21.33
    L1/TEX Hit Rate                        %        10.74
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      2050027
    L2 Hit Rate                            %        11.65
    Mem Pipes Busy                         %        58.71
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.08
    Issued Warp Per Scheduler                        0.60
    No Eligible                            %        39.92
    Active Warps Per Scheduler          warp        12.05
    Eligible Warps Per Scheduler        warp         2.88
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.06
    Warp Cycles Per Executed Instruction           cycle        20.06
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.78
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.51
    Achieved Active Warps Per SM           warp        48.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.058e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 2 excessive sectors (0% of the total      
          18825760 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       856947
    Memory Throughput                 %        20.44
    DRAM Throughput                   %        20.44
    Duration                         us       571.39
    L1/TEX Cache Throughput           %        11.68
    L2 Cache Throughput               %        27.51
    SM Active Cycles              cycle    835771.91
    Compute (SM) Throughput           %        59.39
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.00
    Mem Busy                               %        14.70
    Max Bandwidth                          %        20.44
    L1/TEX Hit Rate                        %         6.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector      1073179
    L2 Hit Rate                            %         6.79
    Mem Pipes Busy                         %        59.39
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.21
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        38.79
    Active Warps Per Scheduler          warp        12.09
    Eligible Warps Per Scheduler        warp         3.01
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.75
    Warp Cycles Per Executed Instruction           cycle        19.76
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.75
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.67
    Achieved Active Warps Per SM           warp        48.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.113e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 2 excessive sectors (0% of the total      
          17848822 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       856069
    Memory Throughput                 %        19.84
    DRAM Throughput                   %        19.84
    Duration                         us       570.78
    L1/TEX Cache Throughput           %        11.60
    L2 Cache Throughput               %        26.98
    SM Active Cycles              cycle    836197.20
    Compute (SM) Throughput           %        59.45
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       974.87
    Mem Busy                               %        14.35
    Max Bandwidth                          %        19.84
    L1/TEX Hit Rate                        %         3.01
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector       522006
    L2 Hit Rate                            %         3.83
    Mem Pipes Busy                         %        59.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.03
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        38.97
    Active Warps Per Scheduler          warp        12.22
    Eligible Warps Per Scheduler        warp         3.01
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        20.02
    Warp Cycles Per Executed Instruction           cycle        20.02
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.73
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.55
    Achieved Active Warps Per SM           warp        48.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.149e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 2 excessive sectors (0% of the total      
          17297640 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  BitonicSort_global(int *, int, int, int) (131072, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       844019
    Memory Throughput                 %        19.81
    DRAM Throughput                   %        19.81
    Duration                         us       562.82
    L1/TEX Cache Throughput           %        11.77
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle    823195.86
    Compute (SM) Throughput           %        60.30
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       973.30
    Mem Busy                               %        14.35
    Max Bandwidth                          %        19.81
    L1/TEX Hit Rate                        %         1.48
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector       254012
    L2 Hit Rate                            %         2.50
    Mem Pipes Busy                         %        60.30
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.99
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        38.01
    Active Warps Per Scheduler          warp        12.24
    Eligible Warps Per Scheduler        warp         3.12
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.75
    Warp Cycles Per Executed Instruction           cycle        19.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    27.72
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -------------
    Metric Name                          Metric Unit  Metric Value
    -------------------------------- --------------- -------------
    Block Size                                                1024
    Cluster Scheduling Policy                         PolicySpread
    Cluster Size                                                 0
    Function Cache Configuration                     CachePreferL1
    Grid Size                                               131072
    Registers Per Thread             register/thread            26
    Shared Memory Configuration Size           Kbyte          8.19
    Driver Shared Memory Per Block       Kbyte/block          1.02
    Dynamic Shared Memory Per Block       byte/block             0
    Static Shared Memory Per Block        byte/block             0
    # SMs                                         SM           132
    Stack Size                                                1024
    Threads                                   thread     134217728
    # TPCs                                                      66
    Enabled TPC IDs                                            all
    Uses Green Context                                           0
    Waves Per SM                                            496.48
    -------------------------------- --------------- -------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.67
    Achieved Active Warps Per SM           warp        49.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     29360128
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.333e-05%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (0% of the total      
          17029568 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

==PROF== Connected to process 2351431 (/home/hice1/mfajeau3/CS8803_GPU/Assigment_2/a.out)
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
==PROF== Profiling "BitonicSort_shared_batched_4x": 0%....50%....100% - 21 passes
[1;32mFUNCTIONAL SUCCESS
[0m[1;34mArray size         :[0m 100000000
[1;34mCPU Sort Time (ms) :[0m 17276.423828
[1;34mGPU Sort Time (ms) :[0m 24930.021484
[1;34mGPU Sort Speed     :[0m 4.011228 million elements per second
[1;31mPERF FAILING
[0m[1;34mGPU Sort is [1;31m1x [1;34mslower than CPU, optimize further!
[1;34mH2D Transfer Time (ms):[0m 0.003904
[1;34mKernel Time (ms)      :[0m 24929.443359
[1;34mD2H Transfer Time (ms):[0m 0.575136
==PROF== Disconnected from process 2351431
[2351431] a.out@127.0.0.1
  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       592296
    Memory Throughput                 %        53.96
    DRAM Throughput                   %        53.96
    Duration                         us       394.91
    L1/TEX Cache Throughput           %        48.99
    L2 Cache Throughput               %        61.36
    SM Active Cycles              cycle    584393.61
    Compute (SM) Throughput           %        44.28
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         2.65
    Mem Busy                               %        48.36
    Max Bandwidth                          %        53.96
    L1/TEX Hit Rate                        %         5.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779332
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        44.28
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        42.78
    Issued Warp Per Scheduler                        0.43
    No Eligible                            %        57.22
    Active Warps Per Scheduler          warp        14.84
    Eligible Warps Per Scheduler        warp         2.00
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.04%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 14.84 active warps per scheduler, but only an average of 2.00 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.68
    Warp Cycles Per Executed Instruction           cycle        34.69
    Avg. Active Threads Per Warp                                27.56
    Avg. Not Predicated Off Threads Per Warp                    25.78
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 31.33%                                                                                          
          On average, each warp of this workload spends 10.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.3% of the total average of 34.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.02
    Achieved Active Warps Per SM           warp        59.53
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.13
    Branch Instructions              inst     16777216
    Branch Efficiency                   %        33.33
    Avg. Divergent Branches                    7943.76
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       618272
    Memory Throughput                 %        68.56
    DRAM Throughput                   %        51.66
    Duration                         us       412.42
    L1/TEX Cache Throughput           %        69.06
    L2 Cache Throughput               %        60.11
    SM Active Cycles              cycle    613185.61
    Compute (SM) Throughput           %        63.02
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         2.54
    Mem Busy                               %        68.56
    Max Bandwidth                          %        63.02
    L1/TEX Hit Rate                        %         0.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778827
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        63.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        59.44
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        40.56
    Active Warps Per Scheduler          warp        14.95
    Eligible Warps Per Scheduler        warp         2.90
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 14.95 active warps per scheduler, but only an average of 2.90 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        25.16
    Warp Cycles Per Executed Instruction           cycle        25.16
    Avg. Active Threads Per Warp                                25.91
    Avg. Not Predicated Off Threads Per Warp                    24.00
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.85
    Achieved Active Warps Per SM           warp        60.07
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst     30408704
    Branch Efficiency                   %        27.27
    Avg. Divergent Branches                   15887.52
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       755804
    Memory Throughput                 %        72.70
    DRAM Throughput                   %        42.27
    Duration                         us       503.94
    L1/TEX Cache Throughput           %        73.18
    L2 Cache Throughput               %        49.67
    SM Active Cycles              cycle    750646.94
    Compute (SM) Throughput           %        68.33
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         2.08
    Mem Busy                               %        72.70
    Max Bandwidth                          %        68.33
    L1/TEX Hit Rate                        %         0.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778807
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        68.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        63.86
    Issued Warp Per Scheduler                        0.64
    No Eligible                            %        36.14
    Active Warps Per Scheduler          warp        15.12
    Eligible Warps Per Scheduler        warp         3.04
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.68
    Warp Cycles Per Executed Instruction           cycle        23.69
    Avg. Active Threads Per Warp                                25.06
    Avg. Not Predicated Off Threads Per Warp                    23.07
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 19.06%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 25.1 threads being active per cycle. This is further reduced  
          to 23.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.85
    Achieved Active Warps Per SM           warp        60.70
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.17
    Branch Instructions              inst     44040192
    Branch Efficiency                   %           25
    Avg. Divergent Branches                   23831.27
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle       910357
    Memory Throughput                 %        74.17
    DRAM Throughput                   %        35.09
    Duration                         us       606.98
    L1/TEX Cache Throughput           %        74.55
    L2 Cache Throughput               %        41.25
    SM Active Cycles              cycle    905529.46
    Compute (SM) Throughput           %        70.70
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.72
    Mem Busy                               %        74.17
    Max Bandwidth                          %        70.70
    L1/TEX Hit Rate                        %         0.23
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778789
    L2 Hit Rate                            %        50.28
    Mem Pipes Busy                         %        70.70
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        65.93
    Issued Warp Per Scheduler                        0.66
    No Eligible                            %        34.07
    Active Warps Per Scheduler          warp        15.27
    Eligible Warps Per Scheduler        warp         3.15
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.16
    Warp Cycles Per Executed Instruction           cycle        23.16
    Avg. Active Threads Per Warp                                24.53
    Avg. Not Predicated Off Threads Per Warp                    22.51
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 20.97%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 24.5 threads being active per cycle. This is further reduced  
          to 22.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.68
    Achieved Active Warps Per SM           warp        61.24
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.18
    Branch Instructions              inst     57671680
    Branch Efficiency                   %        23.81
    Avg. Divergent Branches                   31775.03
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1063638
    Memory Throughput                 %        75.56
    DRAM Throughput                   %        30.03
    Duration                         us       709.34
    L1/TEX Cache Throughput           %        75.93
    L2 Cache Throughput               %        35.42
    SM Active Cycles              cycle   1057398.45
    Compute (SM) Throughput           %        72.52
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.48
    Mem Busy                               %        75.56
    Max Bandwidth                          %        72.52
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16810928
    L2 Hit Rate                            %        50.24
    Mem Pipes Busy                         %        72.52
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        67.26
    Issued Warp Per Scheduler                        0.67
    No Eligible                            %        32.74
    Active Warps Per Scheduler          warp        15.36
    Eligible Warps Per Scheduler        warp         3.11
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.84
    Warp Cycles Per Executed Instruction           cycle        22.84
    Avg. Active Threads Per Warp                                24.18
    Avg. Not Predicated Off Threads Per Warp                    22.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 22.38%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 24.2 threads being active per cycle. This is further reduced  
          to 22.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.24
    Achieved Active Warps Per SM           warp        61.59
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.19
    Branch Instructions              inst     71303168
    Branch Efficiency                   %        23.08
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1182927
    Memory Throughput                 %        73.37
    DRAM Throughput                   %        27.01
    Duration                         us       788.67
    L1/TEX Cache Throughput           %        73.91
    L2 Cache Throughput               %        31.81
    SM Active Cycles              cycle   1174153.53
    Compute (SM) Throughput           %        70.52
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.33
    Mem Busy                               %        73.37
    Max Bandwidth                          %        70.52
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778812
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        70.52
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        67.31
    Issued Warp Per Scheduler                        0.67
    No Eligible                            %        32.69
    Active Warps Per Scheduler          warp        15.42
    Eligible Warps Per Scheduler        warp         3.12
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.91
    Warp Cycles Per Executed Instruction           cycle        22.92
    Avg. Active Threads Per Warp                                24.97
    Avg. Not Predicated Off Threads Per Warp                    22.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 20.26%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 25.0 threads being active per cycle. This is further reduced  
          to 22.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.60
    Achieved Active Warps Per SM           warp        61.82
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.19
    Branch Instructions              inst     80740352
    Branch Efficiency                   %        35.48
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1296358
    Memory Throughput                 %        71.92
    DRAM Throughput                   %        24.64
    Duration                         us       864.42
    L1/TEX Cache Throughput           %        72.22
    L2 Cache Throughput               %        29.21
    SM Active Cycles              cycle   1289779.62
    Compute (SM) Throughput           %        69.31
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.21
    Mem Busy                               %        71.92
    Max Bandwidth                          %        69.31
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16795774
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        69.31
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        67.69
    Issued Warp Per Scheduler                        0.68
    No Eligible                            %        32.31
    Active Warps Per Scheduler          warp        15.47
    Eligible Warps Per Scheduler        warp         3.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                25.62
    Avg. Not Predicated Off Threads Per Warp                    23.36
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 18.71%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 25.6 threads being active per cycle. This is further reduced  
          to 23.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.85
    Achieved Active Warps Per SM           warp        61.98
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst     90177536
    Branch Efficiency                   %        44.44
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1397893
    Memory Throughput                 %        71.33
    DRAM Throughput                   %        22.85
    Duration                         us          932
    L1/TEX Cache Throughput           %        71.73
    L2 Cache Throughput               %        26.98
    SM Active Cycles              cycle   1389923.61
    Compute (SM) Throughput           %        68.77
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.12
    Mem Busy                               %        71.33
    Max Bandwidth                          %        68.77
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778914
    L2 Hit Rate                            %        50.24
    Mem Pipes Busy                         %        68.77
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        68.57
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        31.43
    Active Warps Per Scheduler          warp        15.51
    Eligible Warps Per Scheduler        warp         3.13
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.62
    Warp Cycles Per Executed Instruction           cycle        22.62
    Avg. Active Threads Per Warp                                26.16
    Avg. Not Predicated Off Threads Per Warp                    23.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.17%                                                                                    
          On average, each warp of this workload spends 6.8 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 30.2% of the total average of 22.6 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.57%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 26.2 threads being active per cycle. This is further reduced  
          to 23.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.10
    Achieved Active Warps Per SM           warp        62.14
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst     99614720
    Branch Efficiency                   %        51.22
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1507252
    Memory Throughput                 %        70.51
    DRAM Throughput                   %        21.20
    Duration                         ms         1.01
    L1/TEX Cache Throughput           %        70.86
    L2 Cache Throughput               %        25.14
    SM Active Cycles              cycle   1498611.95
    Compute (SM) Throughput           %        68.58
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.04
    Mem Busy                               %        70.51
    Max Bandwidth                          %        68.05
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778876
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        68.05
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        68.81
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        31.19
    Active Warps Per Scheduler          warp        15.54
    Eligible Warps Per Scheduler        warp         3.04
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.59
    Warp Cycles Per Executed Instruction           cycle        22.59
    Avg. Active Threads Per Warp                                26.62
    Avg. Not Predicated Off Threads Per Warp                    24.22
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.58%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.6% of the total average of 22.6 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.29
    Achieved Active Warps Per SM           warp        62.27
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    109051904
    Branch Efficiency                   %        56.52
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1613111
    Memory Throughput                 %        69.90
    DRAM Throughput                   %        19.81
    Duration                         ms         1.08
    L1/TEX Cache Throughput           %        70.46
    L2 Cache Throughput               %        23.50
    SM Active Cycles              cycle   1600167.14
    Compute (SM) Throughput           %        69.01
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       973.38
    Mem Busy                               %        69.90
    Max Bandwidth                          %        67.47
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16780765
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        67.47
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.36
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        30.64
    Active Warps Per Scheduler          warp        15.57
    Eligible Warps Per Scheduler        warp         3.04
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.45
    Warp Cycles Per Executed Instruction           cycle        22.45
    Avg. Active Threads Per Warp                                27.00
    Avg. Not Predicated Off Threads Per Warp                    24.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.6%                                                                                     
          On average, each warp of this workload spends 7.3 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 32.6% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.48
    Achieved Active Warps Per SM           warp        62.39
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    118489088
    Branch Efficiency                   %        60.78
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1727874
    Memory Throughput                 %        69.03
    DRAM Throughput                   %        18.49
    Duration                         ms         1.15
    L1/TEX Cache Throughput           %        69.40
    L2 Cache Throughput               %        21.86
    SM Active Cycles              cycle   1718554.89
    Compute (SM) Throughput           %        69.03
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       908.68
    Mem Busy                               %        69.03
    Max Bandwidth                          %        66.67
    L1/TEX Hit Rate                        %         0.18
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779016
    L2 Hit Rate                            %        50.19
    Mem Pipes Busy                         %        66.67
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.29
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        30.71
    Active Warps Per Scheduler          warp        15.60
    Eligible Warps Per Scheduler        warp         3.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.52
    Warp Cycles Per Executed Instruction           cycle        22.52
    Avg. Active Threads Per Warp                                27.34
    Avg. Not Predicated Off Threads Per Warp                    24.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.03%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 32.0% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.64
    Achieved Active Warps Per SM           warp        62.49
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    127926272
    Branch Efficiency                   %        64.29
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1841682
    Memory Throughput                 %        68.28
    DRAM Throughput                   %        17.35
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.58
    L2 Cache Throughput               %        20.48
    SM Active Cycles              cycle   1833520.35
    Compute (SM) Throughput           %        69.29
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       852.51
    Mem Busy                               %        68.28
    Max Bandwidth                          %        66.00
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778888
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        66.00
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.65
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.35
    Active Warps Per Scheduler          warp        15.63
    Eligible Warps Per Scheduler        warp         3.21
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.43
    Warp Cycles Per Executed Instruction           cycle        22.44
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32%                                                                                       
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 32.0% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.79
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1847645
    Memory Throughput                 %        68.07
    DRAM Throughput                   %        17.29
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.61
    L2 Cache Throughput               %        20.46
    SM Active Cycles              cycle   1832834.80
    Compute (SM) Throughput           %        69.07
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       849.80
    Mem Busy                               %        68.07
    Max Bandwidth                          %        65.79
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778888
    L2 Hit Rate                            %        50.20
    Mem Pipes Busy                         %        65.79
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.46
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        30.54
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.19
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.49
    Warp Cycles Per Executed Instruction           cycle        22.49
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.86%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.77
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1840316
    Memory Throughput                 %        68.38
    DRAM Throughput                   %        17.36
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.44
    L2 Cache Throughput               %        20.50
    SM Active Cycles              cycle   1837281.48
    Compute (SM) Throughput           %        69.39
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       853.21
    Mem Busy                               %        68.38
    Max Bandwidth                          %        66.09
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779930
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        66.09
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.38
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        30.62
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.19
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.52
    Warp Cycles Per Executed Instruction           cycle        22.52
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.8%                                                                                     
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.8% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1842954
    Memory Throughput                 %        68.23
    DRAM Throughput                   %        17.34
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.43
    L2 Cache Throughput               %        20.48
    SM Active Cycles              cycle   1837536.58
    Compute (SM) Throughput           %        69.24
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       851.97
    Mem Busy                               %        68.23
    Max Bandwidth                          %        65.95
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778888
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        65.95
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.52
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.48
    Active Warps Per Scheduler          warp        15.63
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.48
    Warp Cycles Per Executed Instruction           cycle        22.48
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.94%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.79
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1846743
    Memory Throughput                 %        68.08
    DRAM Throughput                   %        17.30
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.54
    L2 Cache Throughput               %        20.51
    SM Active Cycles              cycle   1834247.77
    Compute (SM) Throughput           %        69.10
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       850.24
    Mem Busy                               %        68.08
    Max Bandwidth                          %        65.81
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778889
    L2 Hit Rate                            %        50.38
    Mem Pipes Busy                         %        65.81
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.56
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.44
    Active Warps Per Scheduler          warp        15.63
    Eligible Warps Per Scheduler        warp         3.19
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.46
    Warp Cycles Per Executed Instruction           cycle        22.47
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.94%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.79
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1844854
    Memory Throughput                 %        68.14
    DRAM Throughput                   %        17.32
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.47
    L2 Cache Throughput               %        20.52
    SM Active Cycles              cycle   1835840.92
    Compute (SM) Throughput           %        69.17
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       851.26
    Mem Busy                               %        68.14
    Max Bandwidth                          %        65.88
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778789
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        65.88
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.70
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.30
    Active Warps Per Scheduler          warp        15.63
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.42
    Warp Cycles Per Executed Instruction           cycle        22.42
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.98%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 32.0% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1836543
    Memory Throughput                 %        68.49
    DRAM Throughput                   %        17.40
    Duration                         ms         1.22
    L1/TEX Cache Throughput           %        68.55
    L2 Cache Throughput               %        20.53
    SM Active Cycles              cycle   1833419.23
    Compute (SM) Throughput           %        69.53
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       854.84
    Mem Busy                               %        68.49
    Max Bandwidth                          %        66.23
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16782778
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        66.23
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.68
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.32
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.42
    Warp Cycles Per Executed Instruction           cycle        22.42
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.91%                                                                                    
          On average, each warp of this workload spends 7.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1837574
    Memory Throughput                 %        68.39
    DRAM Throughput                   %        17.39
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.46
    L2 Cache Throughput               %        20.55
    SM Active Cycles              cycle   1835684.67
    Compute (SM) Throughput           %        69.44
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       854.46
    Mem Busy                               %        68.39
    Max Bandwidth                          %        66.14
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778789
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        66.14
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.65
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.35
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.43
    Warp Cycles Per Executed Instruction           cycle        22.44
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.85%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1843137
    Memory Throughput                 %        68.18
    DRAM Throughput                   %        17.33
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.47
    L2 Cache Throughput               %        20.50
    SM Active Cycles              cycle   1835152.52
    Compute (SM) Throughput           %        69.23
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       851.88
    Mem Busy                               %        68.18
    Max Bandwidth                          %        65.95
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778952
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        65.95
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.59
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.41
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.19
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.45
    Warp Cycles Per Executed Instruction           cycle        22.45
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.76%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.8% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1838419
    Memory Throughput                 %        68.40
    DRAM Throughput                   %        17.38
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.59
    L2 Cache Throughput               %        20.61
    SM Active Cycles              cycle   1831934.70
    Compute (SM) Throughput           %        69.46
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       854.04
    Mem Busy                               %        68.40
    Max Bandwidth                          %        66.16
    L1/TEX Hit Rate                        %         0.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778888
    L2 Hit Rate                            %        50.23
    Mem Pipes Busy                         %        66.16
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.66
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.34
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.43
    Warp Cycles Per Executed Instruction           cycle        22.43
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.81%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.8% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1844218
    Memory Throughput                 %        68.15
    DRAM Throughput                   %        17.32
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.51
    L2 Cache Throughput               %        20.48
    SM Active Cycles              cycle   1834505.98
    Compute (SM) Throughput           %        69.19
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       851.35
    Mem Busy                               %        68.15
    Max Bandwidth                          %        65.91
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16813978
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        65.91
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.60
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.40
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.45
    Warp Cycles Per Executed Instruction           cycle        22.45
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.78%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.8% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1842034
    Memory Throughput                 %        68.26
    DRAM Throughput                   %        17.35
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.69
    L2 Cache Throughput               %        20.53
    SM Active Cycles              cycle   1829365.24
    Compute (SM) Throughput           %        69.32
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       852.48
    Mem Busy                               %        68.26
    Max Bandwidth                          %        66.03
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779132
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        66.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.71
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.29
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.21
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.41
    Warp Cycles Per Executed Instruction           cycle        22.42
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.87%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1839945
    Memory Throughput                 %        68.30
    DRAM Throughput                   %        17.37
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.59
    L2 Cache Throughput               %        20.57
    SM Active Cycles              cycle   1832003.74
    Compute (SM) Throughput           %        69.35
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       853.60
    Mem Busy                               %        68.30
    Max Bandwidth                          %        66.06
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778952
    L2 Hit Rate                            %        50.22
    Mem Pipes Busy                         %        66.06
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.55
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.45
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.19
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.47
    Warp Cycles Per Executed Instruction           cycle        22.47
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.71%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.7% of the total average of 22.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1841053
    Memory Throughput                 %        68.26
    DRAM Throughput                   %        17.35
    Duration                         ms         1.23
    L1/TEX Cache Throughput           %        68.57
    L2 Cache Throughput               %        20.50
    SM Active Cycles              cycle   1832708.95
    Compute (SM) Throughput           %        69.31
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       852.85
    Mem Busy                               %        68.26
    Max Bandwidth                          %        66.02
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779914
    L2 Hit Rate                            %        50.20
    Mem Pipes Busy                         %        66.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.64
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.36
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.20
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.43
    Warp Cycles Per Executed Instruction           cycle        22.44
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.11
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.79%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.8% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1833992
    Memory Throughput                 %        68.52
    DRAM Throughput                   %        17.43
    Duration                         ms         1.22
    L1/TEX Cache Throughput           %        68.63
    L2 Cache Throughput               %        20.63
    SM Active Cycles              cycle   1830909.95
    Compute (SM) Throughput           %        69.58
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       856.63
    Mem Busy                               %        68.52
    Max Bandwidth                          %        66.27
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16779094
    L2 Hit Rate                            %        50.25
    Mem Pipes Busy                         %        66.27
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        69.74
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        30.26
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.21
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.41
    Warp Cycles Per Executed Instruction           cycle        22.41
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    25.10
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.86%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.78
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

  BitonicSort_shared_batched_4x(int *, int, int) (32768, 1, 1)x(1024, 1, 1), Context 1, Stream 13, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         3.20
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      1826694
    Memory Throughput                 %        68.58
    DRAM Throughput                   %        17.49
    Duration                         ms         1.22
    L1/TEX Cache Throughput           %        68.62
    L2 Cache Throughput               %        20.63
    SM Active Cycles              cycle   1825430.95
    Compute (SM) Throughput           %        69.86
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       859.50
    Mem Busy                               %        68.58
    Max Bandwidth                          %        66.54
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector     16778888
    L2 Hit Rate                            %        50.21
    Mem Pipes Busy                         %        66.54
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        70.01
    Issued Warp Per Scheduler                        0.70
    No Eligible                            %        29.99
    Active Warps Per Scheduler          warp        15.62
    Eligible Warps Per Scheduler        warp         3.22
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.32
    Warp Cycles Per Executed Instruction           cycle        22.32
    Avg. Active Threads Per Warp                                27.64
    Avg. Not Predicated Off Threads Per Warp                    23.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.91%                                                                                    
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 31.9% of the total average of 22.3 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 17.66%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 27.6 threads being active per cycle. This is further reduced  
          to 23.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Launch Statistics
    -------------------------------- --------------- -----------------
    Metric Name                          Metric Unit      Metric Value
    -------------------------------- --------------- -----------------
    Block Size                                                    1024
    Cluster Scheduling Policy                             PolicySpread
    Cluster Size                                                     0
    Function Cache Configuration                     CachePreferShared
    Grid Size                                                    32768
    Registers Per Thread             register/thread                24
    Shared Memory Configuration Size           Kbyte            233.47
    Driver Shared Memory Per Block       Kbyte/block              1.02
    Dynamic Shared Memory Per Block      Kbyte/block             16.38
    Static Shared Memory Per Block        byte/block                 0
    # SMs                                         SM               132
    Stack Size                                                    1024
    Threads                                   thread          33554432
    # TPCs                                                          66
    Enabled TPC IDs                                                all
    Uses Green Context                                               0
    Waves Per SM                                                124.12
    -------------------------------- --------------- -----------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           13
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.77
    Achieved Active Warps Per SM           warp        62.58
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.20
    Branch Instructions              inst    137363456
    Branch Efficiency                   %        67.21
    Avg. Divergent Branches                   39718.79
    ------------------------- ----------- ------------

